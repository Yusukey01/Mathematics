<!DOCTYPE html>
<html>
    <head> 
        <title>Orthogonality</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css"> 
    </head>
    <body> 
        <h1>Jacobian</h1>
        <blockquote>
            Consider \(f(x) \in \mathbb{R}^m\), where \(x \in \mathbb{R}^n\). By the differential notation,
            \[
            df = f'(x)dx
            \]
            where \(df \in \mathbb{R}^m \, \), \(dx \in \mathbb{R}^n\) and here, the linear operator \(f'(x)\) is 
            the \(m \times n\) <strong>Jacobian matrix</strong> such that:
            \[
             J_{ij} = \frac{\partial f_i}{\partial x_j}.
            \]
            Here, \(J_{ij}\) represents the rate of change of the \(i\)-th output \(f_i\) with respect to the 
            \(j\)-th input \(x_j\).
            <br>
            For example, Let \(f(x) = \begin{bmatrix}f_1(x) \\ f_2(x) \end{bmatrix}\), 
            and \(x = \begin{bmatrix}x_1 \\ x_2 \end{bmatrix} \). Then
            \[
            df = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} \\
                                 \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} 
                 \end{bmatrix}
                 \begin{bmatrix} dx_1 \\ dx_2 \end{bmatrix}
              =  \begin{bmatrix} \frac{\partial f_1}{\partial x_1}dx_1 + \frac{\partial f_1}{\partial x_2}dx_2 \\
                                 \frac{\partial f_2}{\partial x_1}dx_1 + \frac{\partial f_2}{\partial x_2}dx_2 
                 \end{bmatrix}.
            \]
            As you can see, the Jacobian matrix \(f'(x)\) acts as a linear operator that maps changes in \(x\) 
            encoded in \(dx\) to corresponding changes in \(f(x)\) encoded in \(df\). 
            <br>
            <strong>Note: Input & Output: vector \(\Longrightarrow\) First derivative: matrix(Jacobian).</strong>
            <br><br>
            Instead of expressing the Jacobian component by component, it is often more convenient to use a 
            symbolic representation. 
            <br>
            For example, consider a trivial case: the matrix equation \(f(x) = Ax\) where \(A \in \mathbb{R}^{m \times n}\) 
            is a constant matrix.
            Then 
            \[
            df = f(x + dx) -f(x) = A(x + dx) - Ax = Adx = f'(x)dx
            \]
            Thus \(f'(x) = A \) is the Jacobian matrix. 
            
        </blockquote>

        <h1>Chain Rule</h1>
        <blockquote>
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Chain Rule</span> 
                Suppose \(f(x) = g(h(x))\) where both \(g\) and \(h\) are differentiable. Then
                \[
                df = f'(x)[dx] = g'(h(x))[h'(x)[dx]] 
                \]
            </div>
            Intuitively, if we think about a higher dimensional case, it is clear that the chain rule is not commutative in general. 
            <br>
            For example, consider 
                \[
                x \in \mathbb{R}^n, \, h(x) \in \mathbb{R}^p, \, \text{and } g(h(x)) \in \mathbb{R}^m
                \] 
            Then the output must be the \(m \times n\) <strong>Jacobian matrix</strong> \(f'(x)\).
            <br>
            To construct this \(m \times n\) matrix, we must need the product of the \(m \times p\) Jacobian matrix \(g'(h(x))\) 
            and the \(p \times n\) Jacobian matrix h'(x) in this order. 
        </blockquote>

        <h1>Backpropagation</h1>
        <blockquote>
            The backpropagation or more generally, <strong>reverse mode automatic differentiation</strong> is widely used in 
            machine learning to train <strong>neural networks</strong> by computing gradients of the loss function with respect 
            to the network's parameters such as weights and biases for minimizing the loss(= improving the model's performance).
            <br><br>
            Backpropagation explicitly transmit gradients layer by layer maintaining the order dictated by the 
            <strong>chain rule</strong>. At each layer(= function), the gradient is computed with respect to the 
            output of the previous layer.
            <br>
            Consider a small neural network, which has three layers(functions): \(f_1, f_2, f_3\). This is just a 
            composit function 
            \[
            L(x) = f_3(f_2(f_1(x)))
            \]
            where \(x \in \mathbb{R}^n\) is the inputs(parameters) and \(L(x) \in \mathbb{m}\) is the 
            output called <strong>objective(loss) function</strong>. 
            <br>
            Assume \(f_1 \in \mathbb{p}\), \(f_2 \in \mathbb{q}\), and \(f_2 \in \mathbb{m}\).
            <br>
            Then we need to compute the derivative of this function:
            \[
            dL = f_3'f_2'f_1'
               = (m \times q)(q \times p)(p \times n)
            \]
            This is the multiplication of <strong>Jacobian matrices</strong>. In <strong>reverse</strong> mode the algorithm 
            compute this expression from the left (\(f_3'f_2'\)). So, in the notion of the structure of neural networks, 
            it looks "revers" order, but in mathematical perspective, it is an ordinary left to right calculation. 
            This is critical in a large scale optimization whose lots of inputs(\n\) and only one output(\m =1\).
            \[
            (f_3'f_2')f_1' = [(1 \times q)(q \times p)](p \times n) = (1 \times p)(p \times n) = (1 \times n) = (\nabla L)^T
            \]
            So, each time we only need to compute the product of a row vector(transpose of a gradient) and a Jacobian matrix, which costs 
            \(\Theta(n^2)\). On the other hand, in <strong>forward</strong> mode, we have to compute matrix multipications, which costs \Theta(n^3) 
        </blockquote>

        
        <a href="../index.html">Back to Home </a>
        <br> <a href="calculus.html">Back to Calculus </a>
    </body>
</html>