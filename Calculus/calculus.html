<!DOCTYPE html>
<html>
    <head> 
        <title>Machine Learning</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Calculus</h1>
        <blockquote>
            <p style="text-indent: 40px;">
            <strong>Calculus</strong> is essential in various branches of mathematics, but in this section we will focus on 
            the key areas of calculus that are particularly relevant to machine learning and we assume that readers have already 
            completed Section I. By concentrating on the calculus concepts that directly impact machine learning, we aim to equip 
            readers with the tools needed for optimization, model training, and analysis in the context of machine learning algorithms.
            </p>
        </blockquote>
        <section>
            <h2><a href="linear_approximation.html"><strong>Part 1: Derivative of \(f:\mathbb{R}^n \rightarrow \mathbb{R}\)</strong></a></h2>
                    
                <p><strong>Key words:</strong></p>
                <div class="keywords">
                    <span>Linear approximation</span>
                    <span>Linearization</span>
                    <span>Differentials</span>
                    <span>Product rule</span>
                    <span>Gradient</span>
                    <span>Quadratic form</span>
                    <span>\(L_2\) norm</span>
                </div>
            <h2><a href="jacobian.html"><strong>Part 2: Derivative of \(f:\mathbb{R}^n \rightarrow \mathbb{R}^n\)</strong></a></h2> 
                <p><strong>Key words:</strong></p>
                <div class="keywords">
                    <span>Jacobian matrix</span>
                    <span>Chain rule</span>
                    <span>Backpropagation</span>
                    <span>reverse(forward) mode automatic differentiation</span>
                </div>
            
            <h2><a href="matrix_cal.html"><strong>Part 3: Derivative of \(f:\mathbb{R}^{n \times n} \rightarrow \mathbb{R}^{n \times n}\)</strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Powers of a matrix</span>
                <span>Inverse of a matrix</span>
                <span>LU decomposition</span>
            </div> 
                    
        </section>
        <br>
        <a href="../index.html">Back to Home </a>
    </body>
</html>