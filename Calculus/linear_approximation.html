<!DOCTYPE html>
<html>
    <head> 
        <title>Orthogonality</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css"> 
    </head>
    <body> 
        <h1>Linear Approximations</h1>
        <blockquote>
            <strong>Linear approximation</strong> is a process of approximation a function (\f(x)\) near a point 
            \(x_o\) using a linear function. It simplifies complex functions locally.
            \[
            L(x) = f(x_o) + f'(x_o)(x -x_o) \approx f(x) \tag{1}
            \]
            where \(f'(x_o) = \lim_{x \to x_o} \frac{f(x)-f(x_o)}{x-x_o}\) is derivative at \(x_o\). 
            <br>
            Equivalently, from equation (1), 
            \[
            f(x) - f(x_o) \approx f'(x_o)(x -x_o) 
            \]
            <br><br>
            \(L(x)\) is called the <strong>linearization</strong> of \(f\) at \(x_o\). the graph of \(L\) is 
            the tangent line at \((x_o, f(x_o))\). Linear approximations form the foundation of differentiation 
            and provide a local linear model for \(f(x)\). This concept extends naturally to  differentials.
        </blockquote>

        <h1>Differentials</h1>
        <blockquote>
            <strong>Differentials</strong> describe infinitesimally small changes in quantities. 
            If \(y = f(x)\), where \(f\) is a differentiable function, then the differential \(dx\) is 
            an independent variable and the differential \(dy\) is defined as 
            \[
            dy = f'(x)dx. 
            \]
            In practice, \(dx\) and \(dy\) are arbitrary small numbers. Also, if \(dx \neq 0\), then 
            we recover the familiar derivative form:
            \[
            \frac{dy}{dx} = f'(x)
            \]
            where the left side now represents the ratio of differentials.
            <br><br>
            The exact change in \(y\) corresponding to change in \(x\),(\(dx\)) is given by:
            \[
            \delta y  = f(x + dx) -f(x).  
            \]
            As \(dx \to 0\), the approximation improves
            \[
            dy = f'(x)dx \approx \delta y = f(x + dx) -f(x).
            \] 
            More precisely, the relationship can be written as:
            \[
            f(x + dx) - f(x) = f'(x)dx + o(dx),
            \]
            where \(o(dx)\) is the asymptotic notation, which represents higher-order terms that become negligible as \(dx \to 0\). 
            This highlights that the differential \(dy = f'(x)dx\) serves as a linear approximation to \(\delta y\).
            <br><br>
            More generally, the differential of \(f\) can be expressed as:
            \[
            df = f(x + dx) - f(x) = f'(x)dx,
            \] 
            where \(df\) represents the linearized change in \(f(x)\) due to an infinitesimally small change in \(x\).
            <br>
            Here, \(df\) is the change in the output, \(dx\) is the change in the input, and most importantly, 
            \(f'(x)\) acts as a <strong>linear operator</strong> that maps \(dx\) to \(df\). 
            <br>
            The flexibility of differential notation extends naturally to <strong>linear algebra</strong>, where derivatives 
            apply not only to scalars \(x \in \mathbb{R}\), but also to vectors \(\vec{x} \in \mathbb{R}^n\) and 
            matrices \(X \in \mathbb{R}^{m \times n}\). 
        </blockquote>
        
        <h1>\(f(x) = x^Tx\) where \(x \in \mathbb{R}^n\)</h1>
        <blockquote>
            In this case, the input is the vector \(x\), and the output is the scalar \(x^Tx\).
            To compute the derivative of this function, we start with:
            \[
            f(x) = x^Tx = \sum_{i=1}^n x_i^2 ,
            \]
            where \(x_i\) is the \(i\)-th entry of the vector \(x\).
            Then the <strong>gradient</strong> of \(f\) is:
            \[
            \nabla f = \begin{bmatrix}
                        \frac{\partial f }{\partial x_1} \\ 
                        \frac{\partial f }{\partial x_2} \\ 
                        \vdots \\
                        \frac{\partial f }{\partial x_n}
                        \end{bmatrix}
                     =  \begin{bmatrix} 2x_1 \\ 2x_2\\ \vdots \\  2x_n \end{bmatrix}
                     = 2x
            \]
            Now, let's derive the same result using differential notation. Note: \(dx \in \mathbb{R}^n\).
            <br>
            By the product rule, and the commutativity of the vector inner product:
            \[
            d(x^Tx) = (dx^T)x + x^T(dx) = x^Tdx + x^Tdx = 2x^Tdx.
            \]
            From the differential form, we can identify the gradient:
            \[
            \nabla f = (2x^T)^T = 2x.
            \]
            Note: \(2x^T\) is a "row" vector and to get a column vector \(\nabla f\), we need the transpose of \(2x^T\).

            <br>
        </blockquote>
        <h1>\(f(x) = x^TAx\) where \(x \in \mathbb{R}^n\) and \(A \in \mathbb{R}^{n \times n}\)</h1>
        <blockquote>
            Let's use the differential representation: 
            \[
            df = f(x + dx) -f(x) = (x + dx)^T A (x + dx) - x^T A x
            \]
            Expanding this expression, we get:
            \[
            df = x^TAx + dx^T A x + x^TAdx + dx^T A dx - x^T A x 
            \]
            It is valid to ignore the higher-order term \(dx^T A dx\), which becomes negligible as \(dx \to 0\).
            Then
            \[
            df = dx^TAx + x^TAdx.
            \]
            since \(dx^TAx\) is a scalar, \((dx^TAx)^T = x^TA^Tdx\). Then we get:
            \[
            df = x^TA^Tdx + x^TAdx = x(A+A^T)dx
            \]
            Thus, since \(A+A^T\) is symmetric, 
            \[
            \nabla f = (A+A^T)x.
            \]
        </blockquote>
        <a href="../index.html">Back to Home </a>
        <br> <a href="calculus.html">Back to Calculus </a>
    </body>
</html>