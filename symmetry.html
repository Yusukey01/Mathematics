<!DOCTYPE html>
<html>
    <head> 
        <title>Symmetry</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
    <body> 
        <h1>Symmetric Matrices</h1>
        <blockquote>
            A <strong>symmetric matrix</strong> is a square matrix \(A\) such that
            \[A^T= A\]
            Remember, we learned a least-squares solution \(\hat{\beta}\) satisfies \(X^TX\beta = X^TY\).
            In this expression, \(X^TX\) is always symmetric. Let X be an \(m \times n\) matrix. Then
            \[(X^TX)^T = X^T(X^T)^T = X^TX.\]
            <br>
            An \(n \times n\) matrix \(A\) is <strong>orthogonally diagonalizable</strong> if there an orthogonal matrix
            \(P\) and a diagonal matrix \(D\) such that
            \[A = PDP^T = PDP^{-1}\]
            So, we need \(n\) linearly independent and orthonormal eigenvectors for this diagonalization.
            In this special case, \(A\) is actually symmetric becuase
            \[A^T = (PDP^T)^T = (P^T)^T D^T P^T = PDP^T = A \]
            The converse of this logic is always true as well. If you can see \(A\) is symmetric, immediately you can
            conclude that \(A\) is orthogonally diagonalizable. 
            <br><br>
            <blockquote>
                \(\textbf{Theorem 1:}\)<br>
                 An \(n \times n\) matrix \(A\) is orthogonally diagonalizable if and only if \(A\) is
                 a symmetric matrix. 
            </blockquote><br>
            On Part 6, we used 
            \[A = \begin{bmatrix} 4 & 1 & 1\\ 1 & 4 & 1 \\ 1 & 1 & 4 \\ \end{bmatrix}
             = PDP^{-1} 
             = \begin{bmatrix} -1 & -1 & 1 \\ 0 & 1 & 1 \\ 1 & 0 & 1 \\\end{bmatrix}
               \begin{bmatrix} 3 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 6 \\\end{bmatrix} 
               \begin{bmatrix} \frac{-1}{3} & \frac{-1}{3} & \frac{2}{3} \\
                               \frac{-1}{3} & \frac{2}{3} & \frac{-1}{3}\\
                               \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\\end{bmatrix}
            \] 
            as an example of the diagonalization. Since, \(A\) is symmetric, it must be orthogonally diagonalizable. 
            Here, we have to make \(P\) an orthgonal matrix. To get orthonormal eigenvectors, let's apply the Gram-Schmidt algorithm
            to the columns of \(P\). Since only \(v_2 \cdot v_3 \neq = 0) is the issue, 
            \[v'_3 = v_3 - \frac{v_3 \cdot v_2}{v_2 \cdot v_2}v_2 
                   = \begin{matrix} 1\\ 1\\ 1\\ \end{matrix} - /frac{3}{9}\begin{matrix} 0\\ 3\\ 0\\ \end{matrix}
                   = \begin{matrix} 1\\ 0\\ 1\\ \end{matrix}
            \]
            Finally, by normalization, we get 
            \[ u_1 = \begin{matrix} \frac{-1}{\sqrt{2}} \\ 0\\ \frac{1}{\sqrt{2}} \\ \end{matrix}
               ,\quad
               u_2 = \begin{matrix} 0\\ 1\\ 0\\ \end{matrix}
               \quad \text{, and }
               u_3 = \begin{matrix} \frac{1}{\sqrt{2}} \\ 0\\ \frac{1}{\sqrt{2}} \\ \end{matrix}
            \]




        </blockquote>

        <h1>XX</h1>
        <blockquote>
        </blockquote>
        <h1>XX</h1>
        <blockquote>
        </blockquote>
        <h1>XX</h1>
        <blockquote>
        </blockquote>


        <a href="index.html">Back to Home </a>
        <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
    </body>
</html>