<!DOCTYPE html>
<html>
    <head> 
        <title>Symmetry</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
    <body> 
        <h1>Symmetric Matrices</h1>
        <blockquote>
            A <strong>symmetric matrix</strong> is a square matrix \(A\) such that
            \[A^T= A\]
            Remember, we learned a least-squares solution \(\hat{\beta}\) satisfies \(X^TX\beta = X^TY\).
            In this expression, \(X^TX\) is always symmetric. Let X be an \(m \times n\) matrix. Then
            \[(X^TX)^T = X^T(X^T)^T = X^TX.\]
            <br>
            An \(n \times n\) matrix \(A\) is <strong>orthogonally diagonalizable</strong> if there an orthogonal matrix
            \(P\) and a diagonal matrix \(D\) such that
            \[A = PDP^T = PDP^{-1}\]
            So, we need \(n\) linearly independent and orthonormal eigenvectors for this diagonalization.
            In this special case, \(A\) is actually symmetric becuase
            \[A^T = (PDP^T)^T = (P^T)^T D^T P^T = PDP^T = A \]
            The converse of this logic is always true as well. If you can see \(A\) is symmetric, immediately you can
            conclude that \(A\) is orthogonally diagonalizable. 
            <br><br>
            <blockquote>
                \(\textbf{Theorem 1:}\)<br>
                 An \(n \times n\) matrix \(A\) is orthogonally diagonalizable if and only if \(A\) is
                 a symmetric matrix. 
            </blockquote><br>
            On Part 6, we used 
            \[A = \begin{bmatrix} 4 & 1 & 1\\ 1 & 4 & 1 \\ 1 & 1 & 4 \\ \end{bmatrix}
             = PDP^{-1} 
             = \begin{bmatrix} -1 & -1 & 1 \\ 0 & 1 & 1 \\ 1 & 0 & 1 \\ \end{bmatrix}
               \begin{bmatrix} 3 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 6 \\ \end{bmatrix} 
               \begin{bmatrix} \frac{-1}{3} & \frac{-1}{3} & \frac{2}{3} \\
                               \frac{-1}{3} & \frac{2}{3} & \frac{-1}{3} \\
                               \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \end{bmatrix}
            \] 
            as an example of the diagonalization. Since, \(A\) is symmetric, it must be orthogonally diagonalizable. 
            Here, we have to make \(P\) an orthgonal matrix. To get orthonormal eigenvectors, let's apply the Gram-Schmidt algorithm
            to the columns of \(P\). Since only \(v_1 \cdot v_2 \neq = 0\) is the issue, 
            \[v'_2 = v_2 - \frac{v_2 \cdot v_1}{v_1 \cdot v_1}v_1 
                   = \begin{bmatrix} -1\\ 1\\ 0\\ \end{bmatrix} - \frac{1}{2}\begin{bmatrix} -1\\ 0\\ 1\\ \end{bmatrix}
                   = \begin{bmatrix} \frac{-1}{2}\\ 1 \\ \frac{-1}{2}  \end{bmatrix}
            \]
            By normalization, we get 
            \[ u_1 = \begin{bmatrix} \frac{-1}{\sqrt{2}} \\ 0\\ \frac{1}{\sqrt{2}} \\ \end{bmatrix}
               ,\quad
               u_2 = \begin{bmatrix} \frac{-1}{\sqrt{6}}\\ \frac{2}{\sqrt{6}}\\ \frac{-1}{\sqrt{6}}\\ \end{bmatrix}
               \quad \text{, and }
               u_3 = \begin{bmatrix} \frac{1}{\sqrt{3}} \\  \frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}} \\ \end{bmatrix}
            \]
            Therefore, 
            \[A = \begin{bmatrix} 4 & 1 & 1\\ 1 & 4 & 1 \\ 1 & 1 & 4 \\ \end{bmatrix}
             = PDP^T
             =   \begin{bmatrix} \frac{-1}{\sqrt{2}} & \frac{-1}{\sqrt{6}} & \frac{1}{\sqrt{3}} \\
                                 0                   & \frac{2}{\sqrt{6}}  & \frac{1}{\sqrt{3}}\\
                                 \frac{1}{\sqrt{2}}  & \frac{-1}{\sqrt{6}} & \frac{1}{\sqrt{3}} \end{bmatrix}
               \begin{bmatrix} 3 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 6 \\ \end{bmatrix} 
               \begin{bmatrix} \frac{-1}{\sqrt{2}}   & 0                   & \frac{1}{\sqrt{2}} \\
                               \frac{-1}{\sqrt{6}}   & \frac{2}{\sqrt{6}}  & \frac{-1}{\sqrt{6}}  \\
                               \frac{1}{\sqrt{3}}    & \frac{1}{\sqrt{3}}  & \frac{1}{\sqrt{3}} \end{bmatrix}
            = PDP^{-1}
            \] 
        </blockquote>

        <h1>Quadratic Forms</h1>
        <blockquote>
            A <strong>quadratic form</strong> on \(\mathbb{R}^n\) is a function
            \[Q(x) = x^TAx \, \in \mathbb{R}^n \]
            where \(A\) is an \(n\times n\) <strong>symmetric</strong> matrix. 
            <br><br>
            Let's use our symmetric matrix again
            \[A = \begin{bmatrix} 4 & 1 & 1\\ 1 & 4 & 1 \\ 1 & 1 & 4 \\ \end{bmatrix}\]
            Then 
            \[Q(x) =  x^TAx = 4x_1^2 +4 x_2^2 + 4x_3^2 + x_1x_2 + 2x_2x_3 + 2x_3x_1\]
            <br>
            We can transform this quadratic form into a quadrstic form with no cross-product terms.
            <br>
            Let \(x = Py\) be a change of variable where \(P\) is invertible matrix and \(y\) is a
            new variable in \(\mathbb{R}^n\). Since \(A\) is symmetric, by Theorem 1, \(A\) is
            orthogonally diagonalizable. So, 
            <br>
            \[x^TAx = (Py)^TA(Py) = y^TP^TAPy = y^T(P^TAP)y = y^T(P^TPDP^TP)y = y^TDy \]
            where \(y = P^{-1}x = P^Tx\).
            <br>
            Thus, 
            \[4x_1^2 +4 x_2^2 + 4x_3^2 + x_1x_2 + 2x_2x_3 + 2x_3x_1 = 3y_1^2 + 3y_2^2 + 6 y_3^2\]
            <br>It seemes that eigenvalues of a matrix is deeply connected with its quadratic form. First, 
            we define the classification of quadratic forms.
            <blockquote>
                A quadratic form \(Q(x)\) is
                <ol>
                    <li><strong>positive definite</strong> if \(\, \forall x \neq 0, \, Q(x) > 0\). </li>
                    <li><strong>positive semdefinite</strong> if \(\, \forall x, \, Q(x) \geq 0\). </li>
                    <li><strong>negative definite</strong> if \(\, \forall x \neq 0, \, Q(x) < 0\).</li>
                    <li><strong>negative semdefinite</strong> if \(\, \forall x, \, Q(x) \leq 0\).</li>
                    <li><strong>indefinite</strong> otherwise. </li>
                </ol>
            </blockquote>
            <br>
            <blockquote>
                \(\textbf{Theorem 2:}\)<br>
                 Let \(A\) be an \(n \times n\) symmetric matrix. Then a quadratic form \(x^TAx\) is
                 <ol>
                    <li>positive definite iff the eigenvalues of \(A\) are all positive. </li>
                    <li>negative definite iff  the eigenvalues of \(A\) are all negative.</li>
                </ol>
            </blockquote><br>
            As we mentined above, there is an orthogonal change of variable \(x= Py\) that transforms \(x^TAx\) into 
            \(y^TDy\) with no cross-product terms. 
            \[Q(x)= y^TDy = \lambda_1 y_1^2 + \cdots + \lambda_n y_n^2 \]
            where \(\lambda_1, \cdots, \lambda_n\) are the eigenvalues of \(A\). There is a one-to-one relationship
            between all \(x \neq 0\) and all \(y \neq 0\) because \(P\) is an invertible matrix. So, \(Q(x)\) is 
            determined by the signs of eigenvalues(or <strong>spectrum</strong>) of \(A\) and we call the columns of \(P\)
            the <strong>principle axes</strong> of the quadratic form \(x^TAx\). 
            <br><br>
            Note: To check whether a matrix \(A\) is positive definite or not, it's effective to check existance of 
            <strong>Cholesky factorization</strong> \(A = R^TR\) where \(R\) is some upper triangular matrix with posiitve
            diagonal entries. This is modified version of the <strong>LU factorization</strong>. 
        </blockquote>

        <h1>Singular Value Decomposition(SVD)</h1>
        <blockquote>
        Let \(A\) be an \(m \times n\) matrix. Then the symmetric matrix \(A^TA\) is orthogonally diagonalizable and \(\{v_1, \cdots, v_n\}\)
        be an orthonormal basis for \(\mathbb{R}^n\) consisting of eigenvectors of \(A^TA\) with corresponding eigenvalues 
        \(\lambda_1, \cdots, \lambda_n\). For \(1 \leq i \leq n\),
        \[\| Av_i \|^2 = (Av_i)^TAv_i = v_i^T(A^TAv_i) = v_i^T(\lambda_iv_i) = \lambda_i \geq 0  \tag{1}\]
        From now on, we assume the eigenvalues are arranged so that 
        \[\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n\]
        <br>
        For \(1 \leq i \leq n\), the <strong>singular values</strong> of \(A\) are defined by 
        \[\sigma_i = \sqrt{\lambda_i} = \| Av_i \| \geq 0\]
        and assume \(A\) has \(r\) nonzero singular values, Then \(\{Av_1, \cdots, Av_r\}\) is an 
        orthogonal basis for \(\text{Col }A\) and \(\text{rank } A = \dim \text{ Col }A = r\).
        <blockquote><br>
            \(\textbf{Theorem 3: Singular Value Decomposition}\)<br>
             Let \(A\) be an \(m \times n\) matrix with rank \(r\). Then there exsits an \(m \times n\)
             matrix \(\Sigma = \begin{bmatrix} D & 0 \\ 0 & 0 \\ \end{bmatrix}\) where \(D\) is a \(r \times r\)
             diagonal matrix with the first \(r\) <strong>nonzero</strong> singular values of \(A\), \(\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\)
             and there exist an \(m \times m\) orthogonal matrix \(U\), and an \(n \times n\) orthogonal matrix \(V\) s.t.
             \[A = U\Sigma V^T\]    
        </blockquote><br>
        <blockquote>
            \(\textbf{Proof:}\)<br>
            Normalize orthogonal basis for \(\text{Col }A\) \(\{Av_1, \cdots, Av_r\}\), then we obtain an orthonormal basis
            \(\{u_i, \cdots, u_r\}\), where
            \[u_i = \frac{1}{\| Av_i \|}Av_i = \frac{1}{\sigma_i}Av_i \quad 1 \leq i \leq r\]
            or, 
            \[Av_i = \sigma_i u_i \tag{2}\]
            We extend \(\{u_i, \cdots, u_r\}\) to an orthonormal basis for \(\mathbb{R}^m\), \(\{u_i, \cdots, u_r, u_{r+1}, \cdots, u_m\}\).
            <br>Let \(U = \begin{bmatrix}u_1 & \cdots & u_m \end{bmatrix}\) and \(V = \begin{bmatrix}v_1 & \cdots & v_n \end{bmatrix}\).
            <br>Clearly, by construction, \(U\) and \(V\) are orthogonal matrices. Also, by (2),
            \[AV = \begin{bmatrix} \sigma_1u_1 & \cdots & \sigma_ru_r & 0 & \cdots & 0 \end{bmatrix}.\]
            Let \(D\) be a diagonal matrix with diagonal entries \(\sigma_1, \cdots, \sigma_r\) and 
            let \(\Sigma =\begin{bmatrix} D & 0 \\ 0 & 0 \\ \end{bmatrix}\). Then
            \[U\Sigma = \begin{bmatrix}\sigma_1u_1 & \cdots & \sigma_ru_r & 0 & \cdots & 0\end{bmatrix} = AV\]
            Finally, since \(V\) is an orthogonal matrix, 
            \[(U\Sigma ) V^T = (AV)V^T = AI = A.\]
        </blockquote><br>
        There are three steps for SVD.
        <ol>
            <li>Compute Orthogonal decomposition of \(A^TA\). </li>
                Note: in practice, we should not compute \(A^TA\) since it can be the source of error.
            <li>Construct \(V\) and \(\Sigma\).</li>
                Eigenvalues must be <strong>descending</strong> order and corresponding <strong>unit </strong> eigenvectors are the columns of \(V\).
            <li>Construct \(U\).</li>
                If there are not enough nonzero singular values, we have to find additional orthonormal vectors for
                the colums of \(U\). Set \(u_1^Tx = 0\), find the basis for its solution set, and run the G-S algorithm. 
        </ol>

        <h1>SVD and Fundamental Subspaces</h1>
        <blockquote>

        </blockquote>
       
        <a href="index.html">Back to Home </a>
        <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
    </body>
</html>