<!DOCTYPE html>
<html>
    <head> 
        <title>Orthogonality</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
    <body> 
        <h1>Inner Product & Orthogonality in \(\mathbb{R}^n\) </h1>
        <blockquote>
            The <strong>inner product</strong> of vectors \(u, v \in \mathbb{R}^n \) is a single scalar
            \[u \cdot v = u^Tv = u_1v_1 + \cdots + u_nv_n.\]
            <br>
            We need the definition of the "length" in higher dimension. 
            The <strong>norm</strong> of a vector \(v \in \mathbb{R}^n\) is the nonnegative scalar
            \[ \|v\| = \sqrt{v \cdot v}.\]
            Also, a vector whose length is 1  is called a <strong>unit vector</strong>
            \[u = \frac{v}{\|v\|}\]
            that maintains its "direction." We call this process <strong>normalization</strong>, which
            is useful in many applications such as machine learning. 
            Since we got a "length," now we can define the <strong>distance</strong> between vectors
            \(u, v \in \mathbb{R}^n \) as
            \[dist(u, v) = \|u - v\|. \] 
            Geometrically, \(dist(u, v) \text{ and } dist(u, -v)\) are perpendicular iff they are the same. Consider the squares of the distances.
            \[\|u - v\|^2 = (u-v) \cdot (u-v) = u \cdots (u-v) + (-v) \cdot (u - v) = \|u\|^2 - 2u \cdot v + \|v\|^2\] 
            \[\|u - (-v)\|^2 = (u+v) \cdot (u+v) = u \cdots (u+v) + (v) \cdot (u + v) = \|u\|^2 + 2u \cdot v + \|v\|^2\]
            You might notice that these are close to the Pythagorean theorem:\(\|u  +v \|^2 = \|u\|^2 + \|v\|^2 \) iff 
            \(u\) and \(v\) are orthogonal. So, we need \(- 2u \cdot v = + 2u \cdot v\) for the general orthogonality:
            <br>
            <blockquote>
                Two vectors \(u, v \in \mathbb{R}^n \) are <strong>orthogonal</strong> to each other if their inner product \(u \cdot v =0\). Also,
                if a vector \(v\) is orthogonal to <strong>every</strong> vector in \(W \subseteq \mathbb{R}^n\), then the vector \(v\) is said to be
                orthogonal to \(W\). Moreover, the set of all vectors that are orthogonal to \(W\) is called the <strong>orthogonal complement</strong>
                of \(W\) denoted by \(W^{\perp}\).
            </blockquote>
            The orthogonality allows us to discribe the connection between the column space and the null space of a matrix in another way.
            <br><br>
                <blockquote>
                    \(\textbf{Theorem 1:}\)<br>
                    Let \(A \in \mathbb{R}^{m \times n}\). Then, 
                    \[(Row A)^{\perp} = Nul A \quad \text{ and } \quad (Col A)^{\perp} = Nul A^T\] 
                </blockquote><br>

                <blockquote>
                    \(\textbf{Proof:}\)<br>
                    If \(x \in Nul A\), in other words, \(Ax =0\), \(x\) is orthogonal to each row of \(A\) since \(Ax =0\) means
                    the inner product of each row  of \(A\) and \(x\) are zero. Then, since the rows of \(A\) span the row space of \(A\),
                    \(x\) is orthogonal to \(Row A \). ALso, if \(x\) is orthogonal to \(Row A \), then \(x\) is orthogonal every row of \(A\)
                    and thus \(Ax =0\), or \(x \in Nul A\). Therefore, \((Row A)^{\perp} = Nul A \).
                    <br>Next, since the first equation is true for any matrix, we consider for \(A^T\). Since \(Row A^T = Col A\), 
                    we can say that \((Col A)^{\perp} = Nul A^T \).
                </blockquote>
        </blockquote>

        <h1>The Orthogonal Set</h1>
        <blockquote>
            A set of vectors \(\{v_1, v_2, \cdots, v_p\}\) in \(\mathbb{R}^n\)  is called an <strong>orthogonal set</strong>
            if \(v_i \cdot v_j =0\), \(i \neq j\). Also, an <strong>orthogonal basis</strong> for \(W \subseteq \mathbb{R}^n\) is
            a basis for \(W\) that is orthogonal set. 
        <br><br>
                <blockquote>
                    \(\textbf{Theorem 2:}\)<br>
                    Let \(\{v_1, v_2, \cdots, v_p\}\) be an orthogonal basis for \(W \subseteq \mathbb{R}^n\). For each
                    \(y \in W\), the weights in the linear combination \(y = c_1v_1 + \cdots + c_pv_p\) are given by
                    \[c_j = \frac{y \cdot v_j}{v_j \cdot v_j} \qquad (j = 1, \cdots, p)\]
                </blockquote><br>

                <blockquote>
                    \(\textbf{Proof:}\)<br>
                    Since \(v_1\) is orthogonal to the other vectors in the set, 
                    \[y \cdot v_1 = (c_1v_1 + \cdots + c_pv_p) \cdot v_1 
                                  = c_1(v_1 \cdot v_1) + \cdots + c_p(v_p \cdot v_1)
                                  = c_1(v_1 \cdot v_1).
                    \]
                    By the definition of the basis, \(v_1\) is nonzero \(v_1 \cdot v_1\) must be nonzero, and then we can solve this equation for \(c_1\).
                    Similarly, we can complute \(y \cdot v_j\) and solve for \(c_j \quad (j = 2, \cdots, p)\).
                </blockquote> <br>

            A set \(\{u_1, \cdots, u_p\}\) is said to be an <strong>orthonormal set</strong> if it is an orthogonal set of unit vectors. If \(W\)
            is the subspace spanned by such a set, then \(\{u_1, \cdots, u_p\}\)  is an <strong>orthonormal basis</strong> for \(W\). The matrix 
            whose columns form an orthonormal set is a core in practical applications since such a matrix holds some properties. 
            <br><br>
                <blockquote>
                    \(\textbf{Theorem 3:}\)<br>
                    An matrix \(U \in \mathbb{R}^{m \times n} \) has orthonormal columns iff \(U^TU = I\).   
                </blockquote><br>

                <blockquote>
                    \(\textbf{Proof:}\)<br>
                    \[ U^TU = \begin{bmatrix} u_1^T \\ u_2^T \\  \vdots \\ u_n^T \end{bmatrix}
                              \begin{bmatrix} u_1 & u_2 & \cdots & u_n \end{bmatrix}
                            = \begin{bmatrix} u_1^Tu_1 & u_1^Tu_2 & \cdots & u_1^Tu_n \\
                                              u_2^Tu_1 & u_2^Tu_2 & \cdots & u_2^Tu_n \\
                                              \vdots   &  \vdots  & \ddots & \vdots \\
                                              u_n^Tu_1 & u_n^Tu_2 & \cdots & u_n^Tu_n 
                              \end{bmatrix}
                    \]
                    Then, the columns of \(U\) are orthogonal iff 
                    \[u_i^Tu_j = 0, i \neq j \quad (i, j = 1, \cdots, n).\]
                    Also, the columns of \(U\) have unit length iff
                    \[u_i^Tu_i = 1, (i = 1, \cdots, n).\] 
                    Thus, \(U^TU = I\) and clearly, the converse is also true.
                </blockquote> 
            
            <br><br>
                <blockquote>
                    \(\textbf{Theorem 4:}\)<br>
                    Let \(U \in \mathbb{R}^{m \times n} \) with orthonormal columns, and let \(x, y \in \mathbb{R}^n\).
                    Then
                    <blockquote>
                        <ol>
                            <li>\(\| Ux \| = \| x \| \)</li>
                            <li>\((Ux) \cdot (Uy) = x \cdot y \)</li>
                            <li>\((Ux) \cdot (Uy) = 0\) iff \(x \cdot y = 0\)</li>
                        </ol>
                    </blockquote>
                </blockquote><br>

                <blockquote>
                    \(\textbf{Proof:}\)<br>
                    For property 1, 
                    \[ \| Ux \|^2 = (Ux)^T(Ux) = x^TU^TUx = x^TIx = x^Tx \Longrightarrow \| Ux \| = \| x \| \]
                    For property 2, 
                    \[(Ux) \cdot (Uy) = (Ux)^T(Uy) = x^TU^TUy = x^TIy = x^Ty = x \cdot y \tag{1}\]
                    Note: We can prove properties 1 and 3 from (1).               
                </blockquote><br>
        These theorems are powerful when the matrix is a <strong>square matrix</strong>.
        we call a square invertible matrix \(U\) an <strong>orthogonal matrix</strong> s.t 
        \[U^{-1} = U^T\]
        By Theorem 3, this matrix has <strong>orthonormal</strong> columns(The name is misnomer...). In addition, the rows of \(U\) form 
        an orthonormal basis of \(\mathbb{R}^n\) as well.
        <br>
        For example, the 2D rotation matrix  is an orthogonal matrix. 
         \[R_{\theta} = \begin{bmatrix} \cos \theta & - \sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}\]
         Then 

         \[R_{\theta}^TR_{\theta} = R_{\theta}^{-1}R_{\theta}
                                  = \begin{bmatrix} \cos \theta &  \sin \theta \\ -\sin \theta & \cos \theta \end{bmatrix}
                                     \begin{bmatrix} \cos \theta & - \sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}
                                  =  \begin{bmatrix} \cos^2 \theta + \sin^2 \theta & 0 \\ 0 & \sin^2 \theta + \cos^2 \theta \end{bmatrix}
                                  = I
         \]
         Also, \[\det R_{\theta} = \cos^2 \theta + \sin^2 \theta = 1\] 
         This is actually a property of orthogonal matrices. 
         \[1 = \det I = \det ( U^TU )= \det U^T \det U = \det U \det U = (\det U)^2\]
         (Note: Since \(U\) is a square matrix, \(\det U^T = \det U\).)
         <br>Thus, for any orthogonal matrix \(U\),
         \[\det U = \pm 1\]
         Finally, let's check the eigenvalue of \(R_{\theta}\).

        </blockquote>

        <h1>The Orthogonal Projection</h1>
        <blockquote>
         Given a nonzero vector \(u \in \mathbb{R}^n\). We would like to decompose a vector \(y\in \mathbb{R}^n\)
         into the sum of two vectors such that \[y= \hat{y} + z \tag{2}\] where \(\hat{y} = \alpha u\) is a multiple of \(u\)
         with a scalar \(\alpha\) and \(z\) is orthogonal to \(u\).
         <br>For any scalar \(\alpha\), let \(z = y - \alpha u\) so that (2) is held. Then \(y - \hat{y}\) is 
         <strong>orthogonal</strong> to \(u\) iff
         \[ (y - \alpha u) \cdot u = y \cdot u - \alpha (u \cdot u) = 0\]
         \[\Longrightarrow \alpha  = \frac{y \cdot u}{u \cdot u} \]
         Thus, the <strong>orthogonal projection of \(y\) onto \(u\) </strong> is
         \[\hat{y} = \frac{y \cdot u}{u \cdot u} u\]
         <br><br>
                <blockquote>
                    \(\textbf{Theorem 5: Orthogonal Decomposition Theorem}\)<br>
                    Let \(W \subseteq \mathbb{R}^n\). Then \(\forall y \in \mathbb{R}^n\) can be written uniquely in the form
                    \[y= \hat{y} + z\]
                    where \(\hat{y} \in W\) and \(z \in W^{\perp}\). If \(\{u_1, \cdots, u_p\}\) is any orthogonal basis of \(W\),
                    then 
                    \[\hat{y} = \frac{y \cdot u_1}{u_1 \cdot u_1} u_1 + \cdots + \frac{y \cdot u_p}{u_p \cdot u_p} u_p \tag{3} \] 
                    and \[z = y - \hat{y}.\]
                    Note: (3) is called the <strong>orthogonal projection of \(y\) onto \(W\) </strong> denoted by \(\text{proj}_W y\).
                </blockquote><br>

                <blockquote>
                    \(\textbf{Proof:}\)<br>
                    Let \(\{u_1, \cdots, u_p\}\) is any orthogonal basis of \(W\) and 
                    \(\hat{y} = \frac{y \cdot u_1}{u_1 \cdot u_1} u_1 + \cdots + \frac{y \cdot u_p}{u_p \cdot u_p} u_p\).
                    Since \(\hat{y}\) is a linear combination of \(u_1, \cdots u_p\), \(\quad \hat{y} \in W\). 
                    <br>Let \(z = y - \hat{y}\). Because \(u_i\) is orthogonal to each \(u_j\) in the basis for \(W\) with \(i \neq j\),
                    \[z \cdot u_i = (y -\hat{y}) \cdot u_i = y \cdot u_i - (\frac{y \cdot u_i}{u_i \cdot u_i})u_i \cdot u_i - 0 - \cdots - 0\]
                    \[= y \cdot u_i - y \cdot u_i = 0\]
                    Hence \(z\) is orthogonal to each \(u_i\) in the basis for \(W\). Therefore, \(z\) is orthogonal to every vector in \(W\).
                    In other words, \(z \in W^{\perp}\).
                    <br>Next, suppose \(y\) can also be written as \(y = \hat{y_1} + z_1\) where \(y_1 \in W \text{ and } z_1 \in W^{\perp}\).
                    Then \[\hat{y} +z = \hat{y_1} + z_1 \Longrightarrow \hat{y} - \hat{y_1} = z_1 - z \]
                    This equation implies that the vector \((\hat{y} - \hat{y_1})\) is in both \(W\) and \(W^{\perp}\). Thus,
                    since the only vector that can be  in both \(W\) and \(W^{\perp}\) is the zero vector, \((\hat{y} - \hat{y_1}) = 0\). 
                    Therefore, \(\hat{y} = \hat{y_1} \text{ and } z = z_1\).
                </blockquote><br>

        The orthogonal projection of \(y\) onto \(W\) is <strong>the best approximation to \(y\) by elements of W</strong>. 
        This is significant in practical applications. 
        <br><br>
                <blockquote>
                    \(\textbf{Theorem 6: The Best Approximation Theorem}\)<br>
                    Let \(W \subseteq \mathbb{R}^n\) and \(y \in \mathbb{R}^n\). Also let \(\hat{y}\) be 
                    the orthogonal projection of \(y\) onto \(W\). Then \(\hat{y}\) is the closest point in \(W\) to \(y\) 
                    such that  \[\| y - \hat{y} \| < \| y - v \| \tag{4}\] for all \(v \in W\) distinct from \(\hat{y}\).
                </blockquote><br>

                <blockquote>
                    \(\textbf{Proof:}\)<br>
                    Choose \(v \in W\) distinct from \(\hat{y} \in W\). Then \((\hat{y} - v) \in W\). By the Orthogonal Decomposition
                    Theorem, \((y - \hat{y})\) is orthogonal to \(W\). Moreover,  \((y - \hat{y})\) is orthogonal to \((\hat{y} - v) \in W\)
                    Consider \[y - v = (y - \hat{y}) + (\hat{y} -v) \]. 
                    By the Pythagorean Theorem, 
                    \[\|y - v\|^2 = \|y - \hat{y}\|^2 + \|\hat{y} -v \|^2  > 0\]
                    Thus, we can conclude \(\| y - \hat{y} \| < \| y - v \|\).
                </blockquote><br>

        Finally, the formula (3) becomes much simpler when the basis for \(W\) is an <strong>orthonormal</strong> set as follows:
        <br>If \(\{u_1, \cdots, u_p\}\) is an orthonormal basis for \(W \subseteq \mathbb{R}^n\), then
        \[\text{proj}_W y = (y \cdot u_1)u_1 + \cdots + (y \cdot u_p)u_p\]
        Also, in practice, if \(U = [u_1, \cdots, u_p]\), then
        \[ \forall y \in \mathbb{R}^n, \qquad \text{proj}_W y = UU^Ty\]
        </blockquote>

        <h1>Least Square Problems</h1>
        <blockquote>
        </blockquote>

        <a href="index.html">Back to Home </a>
        <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
    </body>
</html>