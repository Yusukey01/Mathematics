<!DOCTYPE html>
<html>
    <head> 
        <title>Orthogonality</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
    <body> 
        <h1>Inner Product & Orthogonality in \(\mathbb{R}^n\) </h1>
        <blockquote>
            The <strong>inner product</strong> of vectors \(u, v \in \mathbb{R}^n \) is a single scalar
            \[u \cdot v = u^Tv = u_1v_1 + \cdots + u_nv_n.\]
            <br>
            We need the definition of the "length" in higher dimension. 
            The <strong>norm</strong> of a vector \(v \in \mathbb{R}^n\) is the nonnegative scalar
            \[ \|v\| = \sqrt{v \cdot v}.\]
            Also, a vector whose length is 1  is called a <strong>unit vector</strong>
            \[u = \frac{v}{\|v\|}\]
            that maintains its "direction." We call this process <strong>normalization</strong>, which
            is useful in many applications such as machine learning. 
            Since we got a "length," now we can define the <strong>distance</strong> between vectors
            \(u, v \in \mathbb{R}^n \) as
            \[dist(u, v) = \|u - v\|. \] 
            Geometrically, \(dist(u, v) \text{ and } dist(u, -v)\) are perpendicular iff they are the same. Consider the squares of the distances.
            \[\|u - v\|^2 = (u-v) \cdot (u-v) = u \cdots (u-v) + (-v) \cdot (u - v) = \|u\|^2 - 2u \cdot v + \|v\|^2\] 
            \[\|u - (-v)\|^2 = (u+v) \cdot (u+v) = u \cdots (u+v) + (v) \cdot (u + v) = \|u\|^2 + 2u \cdot v + \|v\|^2\]
            You might notice that these are close to the Pythagorean theorem:\(\|u  +v \|^2 = \|u\|^2 + \|v\|^2 \) iff 
            \(u\) and \(v\) are orthogonal. So, we need \(- 2u \cdot v = + 2u \cdot v\) for the general orthogonality:
            <br>
            <blockquote>
                Two vectors \(u, v \in \mathbb{R}^n \) are <strong>orthogonal</strong> to each other if their inner product \(u \cdot v =0\). Also,
                if a vector \(v\) is orthogonal to <strong>every</strong> vector in \(W \subseteq \mathbb{R}^n\), then the vector \(v\) is said to be
                orthogonal to \(W\). Moreover, the set of all vectors that are orthogonal to \(W\) is called the <strong>orthogonal complement</strong>
                of \(W\) denoted by \(W^{\perp}\).
            </blockquote>
            The orthogonality allows us to discribe the connection between the column space and the null space of a matrix in another way.
            <br><br>
                <blockquote>
                    \(\textbf{Theorem 1:}\)<br>
                    Let \(A \in \mathbb{R}^{m \times n}\). Then, 
                    \[(Row A)^{\perp} = Nul A \quad \text{ and } \quad (Col A)^{\perp} = Nul A^T\] 
                </blockquote><br>

                <blockquote>
                    \(\textbf{Proof:}\)<br>
                    If \(x \in Nul A\), in other words, \(Ax =0\), \(x\) is orthogonal to each row of \(A\) since \(Ax =0\) means
                    the inner product of each row  of \(A\) and \(x\) are zero. Then, since the rows of \(A\) span the row space of \(A\),
                    \(x\) is orthogonal to \(Row A \). ALso, if \(x\) is orthogonal to \(Row A \), then \(x\) is orthogonal every row of \(A\)
                    and thus \(Ax =0\), or \(x \in Nul A\). Therefore, \((Row A)^{\perp} = Nul A \).
                    <br>Next, since the first equation is true for any matrix, we consider for \(A^T\). Since \(Row A^T = Col A\), 
                    we can say that \((Col A)^{\perp} = Nul A^T \).
                </blockquote>
        </blockquote>

        <h1>The Orthogonal Set</h1>
        <blockquote>
            A set of vectors \(\{v_1, v_2, \cdots, v_p\}\) in \(\mathbb{R}^n\)  is called an <strong>orthogonal set</strong>
            if \(v_i \cdot v_j =0\), \(i \neq j\). Also, an <strong>orthogonal basis</strong> for \(W \subseteq \mathbb{R}^n\) is
            a basis for \(W\) that is orthogonal set. 
        </blockquote>
        <br><br>
                <blockquote>
                    \(\textbf{Theorem 2:}\)<br>
                    Let \(\{v_1, v_2, \cdots, v_p\}\) be an orthogonal basis for \(W \subseteq \mathbb{R}^n\). For each
                    \(y \in W\), the weights in the linear combination \(y = c_1v_1 + \cdots + c_pv_p\) are given by
                    \[c_j = \frac{y \cdot v_j}{v_j \cdot v_j} \qquad (j = 1, \cdots, p)\]
                </blockquote><br>

                <blockquote>
                    \(\textbf{Proof:}\)<br>
                    
                </blockquote>

        <h1>The Orthogonal Projection</h1>
        <blockquote>
        </blockquote>

        <h1>Least Square Problems</h1>
        <blockquote>
        </blockquote>

        <a href="index.html">Back to Home </a>
        <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
    </body>
</html>