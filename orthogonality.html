<!DOCTYPE html>
<html>
    <head> 
        <title>Orthogonality</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
    <body> 
        <h1>Inner Product & Orthogonality in \(\mathbb{R}^n\) </h1>
        <blockquote>
            The <strong>inner product</strong> of vectors \(u, v \in \mathbb{R}^n \) is a single scalar
            \[u \cdot v = u^Tv = u_1v_1 + \cdots + u_nv_n.\]
            <br>
            We need the definition of the "length" in higher dimension. 
            The <strong>norm</strong> of a vector \(v \in \mathbb{R}^n\) is the nonnegative scalar
            \[ \|v\| = \sqrt{v \cdot v}.\]
            Also, a vector whose length is 1  is called a <strong>unit vector</strong>
            \[u = \frac{v}{\|v\|}\]
            that maintains its "direction." We call this process <strong>normalization</strong>, which
            is useful in many applications such as machine learning. 
            Since we got a "length," now we can define the <strong>distance</strong> between vectors
            \(u, v \in \mathbb{R}^n \) as
            \[dist(u, v) = \|u - v\|. \] 
            Geometrically, \(dist(u, v) \text{ and } dist(u, -v)\) are perpendicular iff they are the same. Consider the squares of the distances.
            \[\|u - v\|^2 = (u-v) \cdot (u-v) = u \cdots (u-v) + (-v) \cdot (u - v) = \|u\|^2 - 2u \cdot v + \|v\|^2\] 
            \[\|u - (-v)\|^2 = (u+v) \cdot (u+v) = u \cdots (u+v) + (v) \cdot (u + v) = \|u\|^2 + 2u \cdot v + \|v\|^2\]
            You might notice that these are close to the Pythagorean theorem:\(\|u  +v \|^2 = \|u\|^2 + \|v\|^2 \) iff 
            \(u\) and \(v\) are orthogonal. So, we need \(- 2u \cdot v = + 2u \cdot v\) for the general orthogonality:
            <br>
            <blockquote>
                Two vectors \(u, v \in \mathbb{R}^n \) are <strong>orthogonal</strong> to each other if their inner product \(u \cdot v =0\). Also,
                if a vector \(v\) is orthogonal to <strong>every</strong> vector in \(W \subseteq \mathbb{R}^n\), then the vector \(v\) is said to be
                orthogonal to \(W\). Moreover, the set of all vectors that are orthogonal to \(W\) is called the <strong>orthogonal complement</strong>
                of \(W\) denoted by \(W^{\perp}\).
            </blockquote>
            The orthogonality allows us to discribe the connection between the column space and the null space of a matrix in another way.
            <br><br>
                <blockquote>
                    \(\textbf{Theorem 1:}\)<br>
                    Let \(A \in \mathbb{R}^{m \times n}\). Then, 
                    \[(Row A)^{\perp} = Nul A \quad \text{ and } \quad (Col A)^{\perp} = Nul A^T\] 
                </blockquote><br>

                <blockquote>
                    \(\textbf{Proof:}\)<br>
                    If \(x \in Nul A\), in other words, \(Ax =0\), \(x\) is orthogonal to each row of \(A\) since \(Ax =0\) means
                    the inner product of each row  of \(A\) and \(x\) are zero. Then, since the rows of \(A\) span the row space of \(A\),
                    \(x\) is orthogonal to \(Row A \). ALso, if \(x\) is orthogonal to \(Row A \), then \(x\) is orthogonal every row of \(A\)
                    and thus \(Ax =0\), or \(x \in Nul A\). Therefore, \((Row A)^{\perp} = Nul A \).
                    <br>Next, since the first equation is true for any matrix, we consider for \(A^T\). Since \(Row A^T = Col A\), 
                    we can say that \((Col A)^{\perp} = Nul A^T \).
                </blockquote>
        </blockquote>

        <h1>The Orthogonal Set</h1>
        <blockquote>
            A set of vectors \(\{v_1, v_2, \cdots, v_p\}\) in \(\mathbb{R}^n\)  is called an <strong>orthogonal set</strong>
            if \(v_i \cdot v_j =0\), \(i \neq j\). Also, an <strong>orthogonal basis</strong> for \(W \subseteq \mathbb{R}^n\) is
            a basis for \(W\) that is orthogonal set. 
        <br><br>
                <blockquote>
                    \(\textbf{Theorem 2:}\)<br>
                    Let \(\{v_1, v_2, \cdots, v_p\}\) be an orthogonal basis for \(W \subseteq \mathbb{R}^n\). For each
                    \(y \in W\), the weights in the linear combination \(y = c_1v_1 + \cdots + c_pv_p\) are given by
                    \[c_j = \frac{y \cdot v_j}{v_j \cdot v_j} \qquad (j = 1, \cdots, p)\]
                </blockquote><br>

                <blockquote>
                    \(\textbf{Proof:}\)<br>
                    Since \(v_1\) is orthogonal to the other vectors in the set, 
                    \[y \cdot v_1 = (c_1v_1 + \cdots + c_pv_p) \cdot v_1 
                                  = c_1(v_1 \cdot v_1) + \cdots + c_p(v_p \cdot v_1)
                                  = c_1(v_1 \cdot v_1).
                    \]
                    By the definition of the basis, \(v_1\) is nonzero \(v_1 \cdot v_1\) must be nonzero, and then we can solve this equation for \(c_1\).
                    Similarly, we can complute \(y \cdot v_j\) and solve for \(c_j \quad (j = 2, \cdots, p)\).
                </blockquote> <br>

            A set \(\{u_1, \cdots, u_p\}\) is said to be an <strong>orthonormal set</strong> if it is an orthogonal set of unit vectors. If \(W\)
            is the subspace spanned by such a set, then \(\{u_1, \cdots, u_p\}\)  is an <strong>orthonormal basis</strong> for \(W\). The matrix 
            whose columns form an orthonormal set is a core in practical applications since such a matrix holds some properties. 
            <br><br>
                <blockquote>
                    \(\textbf{Theorem 3:}\)<br>
                    An matrix \(U \in \mathbb{R}^{m \times n} \) has orthonormal columns iff \(U^TU = I\).   
                </blockquote><br>

                <blockquote>v
                    \(\textbf{Proof:}\)<br>
                    \[ U^TU = \begin{bmatrix} u_1^T \\ u_2^T \\  \vdots \\ u_n^T \end{bmatrix}
                              \begin{bmatrix} u_1 & u_2 & \cdots \vdots  & u_n \end{bmatrix}
                            = \begin{bmatrix} u_1^Tu_1 & u_1^Tu_2 & \cdots & u_1^Tu_n \\
                                              u_2^Tu_1 & u_2^Tu_2 & \cdots & u_2^Tu_n \\
                                              \vdots   &  \vdots  & \vdots & \vdots \\
                                              u_n^Tu_1 & u_n^Tu_2 & \cdots & u_n^Tu_n 
                              \end{bmatrix}
                    \]
                </blockquote> 
            
            <br><br>
                <blockquote>
                    \(\textbf{Theorem 4:}\)<br>
                    Let \(U \in \mathbb{R}^{m \times n} \) with orthonormal columns, and let \(x, y \in \mathbb{R}^n\).
                    Then
                    <blockquote>
                        <ol>
                            <li>\(\| Ux \| = \| x \| \)</li>
                            <li>\((Ux) \cdot (Uy) = x \cdot y \)</li>
                            <li>\((Ux) \cdot (Uy) = 0\) iff \(x \cdot y = 0\)</li>
                        </ol>
                    </blockquote>
                </blockquote><br>

                <blockquote>
                    \(\textbf{Proof:}\)<br>
                    
                </blockquote>
        Done.
        </blockquote>


        <h1>The Orthogonal Projection</h1>
        <blockquote>
        </blockquote>

        <h1>Least Square Problems</h1>
        <blockquote>
        </blockquote>

        <a href="index.html">Back to Home </a>
        <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
    </body>
</html>