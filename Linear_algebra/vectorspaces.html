<!DOCTYPE html>
<html>
    <head> 
        <title>Vector Spaces</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
    <body> 
        <h1>Vector Space</h1>
        <blockquote>
            A <strong>real vector space</strong> is a nonempty set \(V\) of vectors on which are 
            defined <strong>addition</strong> and <strong>scalar multiplication</strong> subject to
            the following eight axioms. 
            
            <br><br>\(\forall u, v, w \in V\) and scalars \(c, d \in \mathbb{R}\), 
            <ol>
                <li>Commutativity of vector addition: \(u + v = v + u\)</li>
                <li>Associativity of vector addition: \((u + v) + w = u + (v + w)\)</li>
                <li>Additive identity element: \(\exists 0 \in V\) s.t. \(u + 0 = u\)</li>
                <li>Additive inverse element: \(\forall u \in V, \exists -u\) s.t. \(u + (-u) = 0\)</li>
                <li>Multiplicative identity: \(1u = u\)</li>
                <li>Distributivity of scalar multiplication wrt vector addition : \(c(u + v) = cu + cv\)</li>
                <li>Distributivity of scalar multiplication wrt scalar addditon: \((c + d)u = cu + du\)</li>
                <li>Compatibility of scalar multiplication: \(c(du)=(cd)u\)</li>      
            </ol>
            <br>
            From now on, in the linear algebra section, "vector space" means "real" vector space. If we need to use
            the <strong>complex vector space</strong> whose scalars \(c, d \in \mathbb{C}\), I will mention it. To understand
            a more general setting, you must know <strong>abstract algebra</strong> ideas, but the current definition is enough for now.
            A simple example of the vector space is \(\mathbb{R}^n\), where \(n \geq 1\). This is why we introduced this concept. 
            <br><br>
            From the definition, \(\forall u \in V \text{and scalar } c\), 
            \[0u =0\]
            \[c0 =0\]
            \[-u =(-1)u\]
            \[cu =0 \Longrightarrow c=0 \text{ or } u =0\]
            <br><br>
            A <strong>subspace</strong> of a vector space \(V\) is a subset \(H\) of \(V\) that has following properties:
            <ol>
                <li>The zero vector of \(V\) is in \(H\)</li>
                <li>\(H\) is closed under vector addition: \(\forall u, v \in H, \quad u+v \in H\)</li>
                <li>\(H\) is closed under scalar multiplication: \(\forall u \in H, \text{ any scalar } c, cu \in H\)</li>
                
            </ol>
            <br>
            For example, a vector space \(\mathbb{R}^2\) is NOT subspace of \(\mathbb{R}^3\) because \(\mathbb{R}^2\) 
            is NOT subset of \(\mathbb{R}^3\).
            <br><br>
            Remember, a <strong>linear combination</strong> is any "sum" of "scalar multiples" of vectors. Also, 
            \(Span\{v_1, v_2, \cdots, v_p\}\) is the set of all linear combinations of \(v_1, v_2, \cdots, v_p\).
            \[\textbf{Theorem} \qquad \text{If } v_1, \cdots, v_p \in V \text{, then } Span\{v_1, \cdots, v_p \} 
            \text{ is a subspace of } V.\]
        </blockquote>

        <h1>Null Space</h1>
        <blockquote>
            A <strong>null space</strong> of \(A \in \mathbb{R}^{m \times n} \) is the set of all solutions of the homogeneous equation \(Ax =0\).
            \[Nul A = \{x: x \in \mathbb{R}^n \text{ and } Ax=0 \in \mathbb{R}^m \}\]
            <br><br>
            So, \(\forall x \in Nul A\) are mapped into the zero vector \(0 \in \mathbb{R}^m\) 
            by the linear transformation \(x \mapsto Ax\). This implies <strong>\(Nul A\) is a subspace of \(\mathbb{R}^n\)</strong>. When we talk about
            a linear transformation \(T: V \mapsto W\) in general setting(it does not have to be a matrix transformation), the null space can be called 
            the <strong>kernel</strong> of \(T\), which is the set of \(\forall u \in V\) s.t. \(T(u)=0 \in W\). 
            <br><br>
            Consider an augmented matrix for \(Ax=0\) and its reduced echelon form.
            \[
            \begin{bmatrix} 2 & 4 & 6 & 8 & 0 \\ 1 & 3 & 5 & 7 & 0 \end{bmatrix} 
            \xrightarrow{\text{rref}} 
            \begin{bmatrix} 1 & 0 & -1 & -2 & 0 \\ 0 & 1 & 2 & 3 & 0 \end{bmatrix} 
            \]
            \(x_3 \text{ and } x_4\) are free variable, and then 
            \[
            \begin{bmatrix} x_1\\ x_2\\ x_3 \\ x_4\\ \end{bmatrix}
            = 
            x_3\begin{bmatrix} 1\\ -2\\ 1 \\ 0\\ \end{bmatrix}
            + 
            x_4\begin{bmatrix} 2\\ -3\\ 0 \\ 1\\ \end{bmatrix}
            \]
            Thus, a <strong>spanning set</strong> for \(NulA\) is 
            \[\left\{
            \begin{bmatrix} 1\\ -2\\ 1 \\ 0\\ \end{bmatrix},
            \begin{bmatrix} 2\\ -3\\ 0 \\ 1\\ \end{bmatrix}
            \right\}
            \]
            , which means that every linear combination of these two vectors is an element of \(Nul A\).
        </blockquote>

        <h1>Column Space</h1>
        <blockquote>
            A <strong>column space</strong> of \(A \in \mathbb{R}^{m \times n} \) is the set of all linear combinations of the 
            columns of \(A = \begin{bmatrix} a_1 & a_2 & \cdots & a_n \end{bmatrix}\).
            \[Col A = Span\{a_1, a_2, \cdots, a_n\} = \{Ax | x \in \mathbb{R}^n \}\]
            <br><br>
            Clearly we can say that <strong>\(Col A\) is a subspace of \(\mathbb{R}^m\)</strong> and it is the range of the
            linear transformation \(x \mapsto Ax\). In addition, \(Col A^T\) is said to be the <strong>row space</strong> of \(A\) denoted by \(Row A\). The row
            space is the set of all linear combinations of the row vectors and it's a subspace of \(\mathbb{R}^n\).
        </blockquote>

        <h1>Basis</h1>
        <blockquote>
            Let \(H\) be a subspace of a vector space \(V\).
            <br>An indexed set of vectors \(\mathcal{B} = \{b_1, \cdots, b_p\} \text{ in } V\) is a <strong>basis</strong> for 
            \(H\) if \(\mathcal{B}\) is a linearly independent set and \(H = Span\{b_1, \cdots, b_p\}\).
            <br><br>
            For example, the set of columns of the identity matrix \(I_n\) is the <strong>standard basis</strong>
            for \(\mathbb{R}^n\). 
            <br>Another simple example is the set of columns of an \(n \times n\) invertible matrix. It forms a basis for \(\mathbb{R}^n\)
            because by the invertible matrix theorem, they are linearly independent and span \(\mathbb{R}^n\).
            <br><br>
            Remember, we found a "spanning" set for \(NulA\):
            \[\left\{
            \begin{bmatrix} 1\\ -2\\  1 \\ 0\\ \end{bmatrix},
            \begin{bmatrix} 2\\ -3\\ 0 \\ 1\\ \end{bmatrix}
            \right\}
            \]
            This set is actually a basis for \(NulA\). However, a basis for \(ColA\) is a litte bit different because
            a basis must be an efficient spanning set(there is no unnecessary vector in the set). Consider again 
            \[
            A = \begin{bmatrix} 2 & 4 & 6 & 8 \\ 1 & 3 & 5 & 7 \end{bmatrix}
                \xrightarrow{\text{ref}} 
                \begin{bmatrix} 1 & 2 & 3 & 4 \\ 0 & 1 & 2 & 3 \end{bmatrix} 
              = B
            \]
            The column \(a_3\) can be generated by \(-a_1 + 2a_2\), and
            also the column \(a_4\) can be generated by \(-2a_1 + 3a_2\).  On the oher hand,
            the <strong>pivot column</strong> of \(B\) is linearly independent and since \(A\) is 
            row equivalent to \(B\), \(a_1 \text{ and } a_2\) are linearly independent as well.
            In general, the pivot column of a matrix \(A\) 
            form a basis for \(Col A\). Thus, in our case, the basis of \(Col A\) is
            \[\left\{ \begin{bmatrix} 2\\ 1\\ \end{bmatrix},
                \begin{bmatrix} 4\\ 3\\ \end{bmatrix} \right\}.
            \]
            <br><br>
            Also, on the above example, \(a_3 = -a_1 + 2a_2\) and \(a_4 = -2a_1 + 3a_2\) are "unique" representation. 
            <br>
            <blockquote>
                \(\textbf{Theorem 1: The Unique Representation Theorem}\)<br>
                Let \(\mathcal{B} = \{b_1, \cdots, b_n\}\) be a basis for a vector space \(V\).
                Then, there exsits a unique set of scalars \(c_1, \cdots, c_n\) such that
                \[x = c_1b_1 + \cdots + c_nb_n.\]
            </blockquote>
            <br>
            \(\textbf{Proof:}\)<br>
            Since \(\mathcal{B}\) spans \(V\), there exist a set of scalars such that 
            \[x = c_1b_1 + \cdots + c_nb_n \tag{1}\] 
            Now, assume that \(x\) also has another set of scalars such that 
            \[x = d_1b_1 + \cdots + d_nb_n \tag{2}\] Then (1)-(2), 
            \[x - x = (c_1 - d_1)b_1 + \cdots + (c_n-d_n)b_n\]
            To hold this equation, for \(1 \leq i \leq n\), the weights \(c_i - d_i\) must all
             be zero because \(\mathcal{B}\) is linearly independet set. 
            Thus, \(c_i = d_i\).
        </blockquote>

        <h1>Coordinate Systems</h1>
        <blockquote>
            Suppose \(\mathcal{B} = \{b_1, \cdots, b_n\}\) is a basis for a vector space \(V\) and \(x \in V\).
            The <strong>\(\mathcal{B}\)-coordinates of \(x\)</strong> are weights \(c_1, \cdots, c_n \) such that
            \(x = c_1b_1 + \cdots + c_nb_n\). Moreover, the vector 
            \[[x]_{\mathcal{B}} = \begin{bmatrix} c_1\\ \cdots \\  c_n \\ \end{bmatrix} \]
            is called the <strong>coordinate vector of \(x\)</strong>. The mapping \(x \mapsto [x]_{\mathcal{B}}\) is
            the <strong>coordinate mapping</strong> that is a <strong>one-to-one linear</strong> transformation from \(V\) onto \(\mathbb{R}^n\).
            <br><br>
            The coordinate mapping is <strong>isomorphism</strong> from \(V\) onto \(\mathbb{R}^n\).
            It allows us to analize an "unfamiliar" vector space \(V\) via "familiar" vector space \(\mathbb{R}^n\). 
            all operations in \(V\) is preserved in \(\mathbb{R}^n\).
        </blockquote>

        <h1>Rank</h1>
        <blockquote>
          The <strong>dimension</strong> of a vector space \(V\) is the number of vectors in a basis for \(V\).
          <br><br>
          The <strong>rank</strong> of a matrix \(A\) is the dimension of the column space of \(A\).
          \[\text{rank }A = \text{dim }Col A = \text{ the number of pivot columns of A}\]
          <br>
          Note that since \(Row A = Col A^T\), \(\text{dim }Row A = \text{rank }A^T\). Also, the \(\text{dim }Nul A \) is 
          the number of free variable in \(Ax=0\), in other words, it is the number of nonpivot colums of \(A\).
          Thus, for an\(m \times n\) matrix \(A\), \[\text{rank }A + \text{dim }Nul A = n. \]
        </blockquote>

        <a href="../index.html">Back to Home </a>
        <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
    </body>
</html>