<!DOCTYPE html>
<html>
    <head> 
        <title>Trace</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css"> 
    </head>
    <body> 
        <h1>Trace and Norms</h1>
        <blockquote>
            The <strong>trace</strong> of an \(n \times n\) matrix \(A\) is the sum of the diagonal entries of \(A\) and
            denoted by \(\text{tr }(A)\).
            \[\text{tr }(A) =\sum_{k=1}^n a_{kk} \]
            We list some properties of the trace. 
            <ol>
                <li>\(\text{tr }(cA) = c (\text{tr }A) \quad , c \in \mathbb{R}\)</li>
                <li>\(\text{tr }(A+B) = \text{tr }A + \text{tr }B  \quad , B \in \mathbb{R}^{n \times n}\)</li>
                <li>\(\text{tr }A^T = \text{tr }A \)</li> <br>
                <li>\(\text{tr }(A^TB) = \sum_{i=1}^m\sum_{j=1}^n a_{ij}b_{ij}\)</li> <br>
                <li>\(\text{tr }(AB) = \text{tr }(BA) \)</li>
                Note: \(A\) can be \(m \times n\) and  \(B\) can be \(n \times m\). More generally, <br>
                \(\text{tr }(ABC) = \text{tr }(CAB) = \text{tr }(BCA) \): <strong>cyclic permutation property</strong> <br><br>
                <li>\(\text{tr }A =\sum_{i=1}^n \lambda_i \) where \(\lambda_1, \cdots, \lambda_n\) are the eigenvalues of \(A\)</li>
            </ol>
            For example, consider a quadratic form \(x^TAx \in \mathbb{R}\). <a href="symmetry.html"><strong> Check: Part 9</strong></a>
            <br>Let \(x\) be a vector in \(\mathbb{R}^n\) by Property 1 and 5, 
            \[x^TAx = \text{tr }(x^TAx) = \text{tr }(xx^TA) = (x^Tx)\text{tr }(A)\]
            <br><br>
            Property 4 is actually the definition of the <strong>inner product</strong> for matrices. 
            <a href="orthogonality.html"><strong>Check: Part 7 </strong></a>
            <br>We call it the <strong>Frobenius inner product</strong>.  For matrices \(A, B \in \mathbb{R}^{m \times n}\),
            \[\left\langle A, B \right\rangle_F = \text{tr }(A^TB) = \sum_{i=1}^m\sum_{j=1}^n a_{ij}b_{ij}\]
            Also, like the Euclidean(\(l_2\)) norm \(\|v\|_2 = \sqrt{v \cdot v} = v^Tv\) of vector, we define 
            the <strong>Frobenius norm</strong> of a "matrix" \(A\):
            \[\| A \|_F = \sqrt{\left\langle A, A \right\rangle_F} = \sqrt{\text{tr }(A^TA)}\] 
            <br><br>
            Norms play a key role in <strong>normalization</strong> and <strong>regularization</strong>. Depending on the
            application, we choose an appropriate norm. Another popular norm of a matrix is the <strong>nuclear norm(trace norm)</strong>. 
            <br>Given a matrix \(A \in \mathbb{R}^{m \times n}\), then the nuclear norm of \(A\) is defined as the sum of its
            <strong>singular values</strong>.
            \[\| A\|_{\ast} = \sum_{i} \sigma_i \geq 0\]
            <div class="theorem">
                <span class="theorem-title">Theorem 1:</span>  
                Let a matrix \(A = U\Sigma V^T\) by SVD. Then
                \[\| A\|_{\ast} = \sum_{i} \sigma_i = \text{tr }(\Sigma) = \text{tr }(\sqrt{A^TA})\].
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Suppose \(A = U\Sigma V^T\) by the singular value decomposition. 
                Since\(A^TA = (V\Sigma U^T)(U\Sigma V^T) = V\Sigma^2 V^T\), 
                \[\sqrt{A^TA} = V\Sigma V^T\]
                Then by the cyclic permutation property of trace, 
                \[\text{tr }(\sqrt{A^TA}) = \text{tr }(V\Sigma V^T) = \text{tr }(\Sigma V^T V) = \text{tr }(\Sigma ) \]
            </div>
            <br>
            Finally, the <strong>induced norm</strong> of \(A\) is defined as
            \[\| A \|_p = \max_{x \neq 0} \frac{\| Ax \|_p}{\| x \|_p} = \max_{\| x \| =1} \| Ax \|_P \tag{1}\]
            Typically, when \(p =2\), we call it the <strong>spectral norm</strong> of \(A\), which is just
            the largest singular value of \(A\).
            \[\| A \|_2 = \max_{i} \sigma_i  = \sqrt{\lambda_{\max} (A^TA)}\] 
            where \(\lambda_{max}\) is the largest eigenvalue of \(A\). 
            <br>The spectral norm is commonly used to control the <strong>Lipschiz continuity</strong> of
            of functions in the <strong>neural network</strong> in order to prevent <strong>vanishing or exploding
            gradients</strong>.
            <br><br>
            \(\| x \|_p \) in (1) is called the <strong>\(p\)-norm</strong> of the vector \(x\). The Euclidean norm is 
            a specific case of \(p\)-norm where \(p =2\). In general, 
            \[\| x \|_p = (\sum_{i =1} ^n |x_i|^p)^{\frac{1}{p}} \quad , p \geq 1\]
            When \(p =1\), we call it <strong>Manhattan norm</strong>, which is commonly used in the grid-based
            environments. Also, if we need <strong>sparsity</strong> in models such as Lasso regression, the Manhattan
            norm is used as a regularizer. 
            <br>Moreover, when \(p\) approachies \(\infty\), we define the <strong>maximum norm</strong>: 
            \[\| x \|_\infty = \lim_{p \to \infty} \| x \|_p = \max_{i} |x_i|\]
            In numerical analysis, the maximum norm is used for "worst-case" error analysis. Similarly,
            in machine learning, it is used when we want to minimize the worst error rather than the total
            error across sample deta.
        </blockquote>

        <h1>Norms in Machine Learning</h1>
        <blockquote>
            In machine learning, norms are fundational for ensuring consistent scaling, constraining comlexity 
            and maintaining training stability of models. <br>
            Norms are used to scale data to a "common" range. It ensures that every single feature contributes
            "equally" to the training of the model and reduces sensitivity to the scale of each feature. This
            <strong>normalization</strong> process is important in the "distance(= norm)" based models such as 
            <strong>support vector machines(SVMs)</strong> and <strong>\(k\)-nearest neighbor(k-NN)</strong>. 
            Also, <strong>regularization</strong> uses norms to "penalize" the model complexity, which avoids 
            overfitting and then helps the model <strong>generalize</strong> to new data.
        </blockquote>

        <h1>Norms in Metric Spaces</h1>
        <blockquote>

        </blockquote>




        <a href="../index.html">Back to Home </a>
        <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
    </body>
</html>