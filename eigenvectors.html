<!DOCTYPE html>
<html>
    <head> 
        <title>Eigenvectors</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
    <body> 
        <h1>Eigenvectors and Eigenvalues</h1>
        <blockquote>
            An <strong>eigenvector</strong> of an \(n \times n\) matrix \(A\) is a "nonzero" vector \(x\) s.t.
            \[Ax = \lambda x \tag{1}\]
            for some scalar \(\lambda\) that is called an <strong>eigenvalue</strong> of \(A\) if there is a
            nontrivial solution \(x\) of (1). We call such an \(x\) an eigenvector corresponding to \(\lambda\).
            Also, (1) gives us
            \[(A - \lambda I)x = 0 \tag{2}\] 
            \(\lambda\) is an eigenvalue of \(A\) iff (2) has a nontrivial solution, then set of all solutions of
            (2) is the \(Nul (A - \lambda I) \subseteq \mathbb{R}^n\) and is called the <strong>eigenspace</strong> of \(A\)
            corresponding to the eigenvalue \(\lambda\).
            <br><br>
            <blockquote>
                \(\textbf{Theorem 1}\)<br>
                The eigenvalues of a triangular matrix are its main diagonal entries. 
            </blockquote><br>
                Suppose \(A \in \mathbb{R}^{3 \times 3} \) is a lower triangular matix and \(\lambda\) is an
                eigenvalue of \(A\). Then, 
                \[A - \lambda I = \begin{bmatrix} 
                  a_{11} - \lambda & 0 & 0 \\ 
                  a_{21} & a_{22} - \lambda & 0 \\
                  a_{31} & a_{32} & a_{33} - \lambda \\
                  \end{bmatrix}
                \]
                Since \(\lambda\) is an eigenvalue of \(A\), \((A-\lambda I)x =0\) has a nontrivial solution. In other words,
                the equation has a free variable. In this case, it will happen iff at least one of the main diagonal entries is zero
                so that \(\lambda\) is equal to one of the main diagonal entries of \(A\). Similarly, this is true for the upper triangular case
                and any higher dimensional cases. 
                <br><br>
                For example, 
                \[A = \begin{bmatrix} 0 & 1 & 8 \\  0 & 2 & 7 \\ 0 & 0 & 3 \\ \end{bmatrix}\]
                has eigenvalues; 0, 2, and 3. Since \(A\) has a zero eigenvalue, the equation (1) becomes
                homogeneous equation \(Ax =0\), which must have a nontrivil solution and it happens iff 
                <strong>\(A\) is a singular matrix(NOT invertible).</strong> We can double check it by computing
                \(det A = 0(6-0)-(0-0)+8(0-0)=0\). Since \(detA=0\), clearly, \(A\) is not invertible. 
                <br><br>
                <blockquote>
                    \(\textbf{Theorem 2}\)<br>
                    If \(v_1, \cdots, v_n\) are eigenvectors corresponding to "distinct" eigenvalues \(\lambda_1, \cdots, \lambda_n\)
                    of a square matrix \(A\), then the set \(\{v_1, \cdots, v_n\}\) is linearly independent.
                </blockquote><br>

                <blockquote>
                    \(\textbf{Proof:}\)<br>
                    Assume the set is linearly "dependent." By the definition, the eigenvector \(v_1\) is nonzero.
                    Then, one of the eigenvectors must be a "linear combination" of preceding eigenvectors.
                    Let \(i\) be the least index so that \(v_{i+1}\) is a linear combination of the preceding eigenvectors.
                    Then there exist scalars \(c_1, \cdots, c_i\) s.t.
                    \[c_1v_1 + \cdots + c_iv_i = v_{i+1} \tag{3}\]
                    Multiplying its both sides by a suquare matrix\(A\), we get
                    \[c_1Av_1 + \cdots + c_iAv_i = Av_{i+1}\]
                    and by the definition of eigenvalues & eigenvectors, 
                    \[c_1 \lambda_1 v_1 + \cdots + c_i \lambda_i v_i = \lambda_{i+1} v_{i+1}. \tag{4}\]
                    Now, by (4) - (\(\lambda_{i+1} \times (3) )\), we obtain
                    \[c_1(\lambda_1 - \lambda_{i+1}) v_1 + \cdots + c_i(\lambda_i - \lambda_{i+1} )v_i = 0. \tag{5}\]
                    The weights of (5) must be all zero since \(\{ v_1, \cdots, v_i\}\) is linerarly independent.
                    However, none of \((\lambda_1 - \lambda_{i+1}), \cdots, (\lambda_i - \lambda_{i+1} )\) are zero because
                    the eigenvalues are "distinct." Thus, all scalars \(c_1, \cdots, c_i\) have to be zero but this 
                    is impossible because of the equation (3). Therefore, the set cannot be linearly dependent, so it is a
                    linearly independent set. 
                </blockquote><br>
        </blockquote>

        <h1>Characteristic equations</h1>
        <blockquote>
            A scalar \(\lambda\) is an eigenvalue of an \(n \times n\) matrix \(A\) iff \(\lambda\) satisfies
            the <strong>characteristic equation</strong> 
            \[det (A - \lambda I) = 0.\]
            For example, consider a matrix 
            \[A = \begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 4 \\ 5 & 6 & 0 \\ \end{bmatrix}
            \]
            Then, 
            \[det(A- \lambda I) = (1-\lambda)(-\lambda^2 -\lambda -24) -0 +5(5 +3\lambda) = 0 \]
            We obtain 
            \[-\lambda^3 +2\lambda^2 + 38\lambda +1 = 0\]
            Now we can solve for \(\lambda\) to get eigenvalues for \(A\): \(\lambda \approx -5.230, -0.026, 7.256\).
            Like this example, usually we can only approximate eiganvalues by numerical methods in practice.
            <br><br>
            The <strong>algebraic multiplicity</strong> of an eigenvalue is its multiplicity as a root of the 
            characteristic equation.<br><br>
            For example, if we get a <strong>characteristic polynomial</strong> of a matrix; \((\lambda -1)^2 (\lambda -2) = 0\), 
            the eigenvalue 1 is said to have multiplicity 2. Now, we would like to introduce <strong>similarity</strong>
            of matrices.<br>
            Suppose \(A\) and \(B\) are \(n \times n\)matrices. Then, \(A\) is similar to \(B\) if there is an invertible
            matrix \(P\) such that 
            \[P^{-1}AP =B \text{, or equivalently, } A=PBP^{-1}.\]
            <br>
            <blockquote>
                \(\textbf{Theorem 3}\)<br>
                "If" \(n \times n\) matrices \(A\) and \(B\) are similar, "then" they have the same characteristic
                polynomial and thus the same eigenvalues with the same multiplicities.
                <br>
                Note: the converse of this theorem is not true. Having the same eigenvalues does not mean the matrices are similar.
            </blockquote><br>

            <blockquote>
                \(\textbf{Proof:}\)<br>
                If \(B = P^{-1}AP\), then 
                \[B-\lambda I = P^{-1}AP -\lambda P^{-1}P = P^{-1}(A-\lambda I)P\]
                and by multiplicative property of determinants,
                \[det(B-\lambda I) = det(P^{-1}) det(A-\lambda I) det(P) = det(A-\lambda I).\]
                Note: \(det(P^{-1}P)=det(I) =1\).
            </blockquote>
        </blockquote>

        <h1>Diagonalization</h1>
        <blockquote>
            A square matrix \(A\) is said to be <strong>diagonalizable</strong> if for some invertible matrix \(P\),
            \(A\) is similar to a diagonal matrix \(D\). \[A = PDP^{-1}\]
            <blockquote>
                \(\textbf{Theorem 4: Diagonalization}\)<br>
                An \(n \times n\) matrix \(A\) is diagonalizable iff \(A\) has \(n\) linearly independent eigenvectors.
                So, the columns of \(P\) are linearly independent eigenvectors of \(A\) and the diagonal entries of \(D\)
                are eigenvalues of \(A\) corresponding to the eigenvectors in \(P\). 
                Note: \(P\) and \(D\) are not unique because we can change the oder of the diagonal entries.
            </blockquote><br>
            <blockquote>
                \(\textbf{Proof:}\)<br>
                First, let \(P\) be a square matrix with columns \(v_1, \cdots, v_n\) and \(D\) be a diagonal matrix with
                diagonal entries \(\lambda_1, \cdots, \lambda_n \). Then, 
                \[AP = \begin{bmatrix}Av_1 & \cdots & Av_n \\\end{bmatrix}\]
                \[PD = \begin{bmatrix}\lambda_1 v_1 & \cdots & \lambda_n v_n \\\end{bmatrix}\]
                Assume \(A\) is diagonalizable and \(A = PDP^{-1}\). Then right multiplying this 
                equation by \(P\), we get \(AP = AD\).
                Thus, for each clumn of them,
                \[Av_1 = \lambda_1 v_1, \cdots, Av_n = \lambda_n v_n. \tag{1}\]
                Since \(P\) is invertible, its colums are linearly independent and must be nonzero. Hence, by (1),
                \(\lambda_1, \cdots, \lambda_n \) are eigenvalues and \(v_1, \cdots, v_n\) are corresponding eigenvectors.
                <br><br>
                Finally, consider any \(n\) eigenvectors \(v_1, \cdots, v_n\). We can construct \(P\) and \(D\) by  them and its 
                corresponding eigenvalues \(\lambda_1, \cdots, \lambda_n \). Then, again we get \(AP = PD\). If the eigenvectors
                are linearly independent, then \(P\) must be invertible, and thus, we obtain \(A = PDP^{-1}\).   
            </blockquote>
         <br><br>
         For example, given
         \[A = \begin{bmatrix} 4 & 1 & 1\\ 1 & 4 & 1 \\ 1 & 1 & 4 \\ \end{bmatrix}\] 
         then 
         \[det(A- \lambda I) = (4-\lambda)((4-\lambda)^2 - 1) -((4-\lambda )-1)+(1-(4 -\lambda)) = 0 \]
        Simplify this, 
        \[-\lambda^3 +12\lambda^2 -45\lambda -54 = 0\]
        \[(\lambda - 3)^2(\lambda -6) = 0\]
        Thus, we obtain eigenvalues \(\lambda_1 = 3\), \(\lambda_2 = 3\), \(\lambda_3 = 6\). Next, we need three linearly
        independent eigenvectors corresponding to each eigenvalue. 


        </blockquote>

        <h1>X</h1>
        <blockquote>
        </blockquote>

        <h1>X</h1>
        <blockquote>
        </blockquote>

        <a href="index.html">Back to Home </a>
        <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
    </body>
</html>