<!DOCTYPE html>
<html>
    <head> 
        <title>Linear Transformation</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
    <body> 
        <h1>Linear Transformation</h1>
        <blockquote>
            A transformation (or mapping) \(T: \mathbb{R}^m  \rightarrow \mathbb{R}^n \) is <strong> linear</strong> if<br> 
            \(\forall u, v \in\mathbb{R}^m,\) and any scalars \(c\),
            \[T(u+v) = T(u) +T(v)\]
            \[T(cu) = cT(u).\]
           <br>
           For example, consider a matrix \(A \in \mathbb{R}^{m \times 3}\) and a vector \(x \in \mathbb{R}^3 \). 
           Then, the matrix-vector product \(Ax\) is actually a linear transformation \(T: x \mapsto Ax\).  
           Say, \(u, v \in \mathbb{R}^3 \) and \(c\) is a scalar. Let's check the two conditions: 
           \[
           A(u+v) = 
           \begin{bmatrix}
           a_1 & a_2 & a_3 
           \end{bmatrix}
           \begin{bmatrix}
           u_1 + v_1 \\
           u_2 + v_3 \\
           u_3 + v_3 
           \end{bmatrix}
           = (u_1 + v_1)a_1 + (u_2 + v_2)a_2 + (u_3 + v_3)a_3
           = (u_1a_1 + u_2a_2 + u_3a_3) + (v_1a_1 + v_2a_2 + v_3a_3)
           = Au + Av
           \]

           \[
           A(cu) = 
           \begin{bmatrix}
           a_1 & a_2 & a_3 
           \end{bmatrix}
           \begin{bmatrix}
           cu_1\\
           cu_2 \\
           cu_3
           \end{bmatrix}
           = c(u_1a_1)+c(u_2a_2)+c(u_3a_3)
           = c(u_1a_1 + u_2a_2 + u_3a_3)
           = c(Au).
           \]
           In general, the operations of vector addition and scalar multiplication are preserved under linear transformations.
           Also, we can say that if a transformation \(T\) is linear, then 
           \[T(0)=0\] and \[T(au + bv)=aT(u)+bT(v)\] where \(a, b\) are scalars and \(u, v\) is in the domain of \(T\).
           <br>
           <br>
           Essentially, we are not only tallking about vectors and matrices. The linear transformation is one of the most fundamental idea in mathematics. 
           You have seen this essential conceptA multiple times outside Linear Algebra. Let's briefly observe some examples.
           <br><br>
           A function \(f(x) = mx\) is linear transformation \(f:\mathbb{R} \rightarrow \mathbb{R}\).
           \[f(ax+by)=m(ax+by)=a(mx)+b(my) = af(x) + bf(y)\]
           Note: the grapth of fanctions that have linearity must pass through the origin 
           since lienar transformations must satisfy \(T(0) =0\).
           <br>
           If you have just started to learn calculus, you might know that
           \[\frac{d}{dx}(aX(t)+bY(t)) =a\frac{d}{dx}(X(t))+b\frac{d}{dx}(Y(t)).\]
           For example, 
           \[\frac{d}{dx}(5x^3 +4x^2) = 15x^2 + 8x  =5\frac{d}{dx}(x^3)+4\frac{d}{dx}(x^2)\]
           so the differential is just a <strong>linear operator</strong>. (actually, the integration too.)
           <br><br>
           Have you learned statistics? The expected value also has linearity. For any random variables \(X\) and \(Y\), and constants \(a\) and \(b\)
           \[\mathbb{E}[aX+bY]=a\mathbb{E}[X]+b\mathbb{E}[X]\].
        </blockquote>

        <h1>Matrix Multiplication</h1>
        <blockquote>
            Suppose \(A\) is an \(m \times n\) matrix and \(B\) is an \(n \times p\) matrix , then the product \(AB\)
            is the \(m \times p\) matrix 
            \[
            AB = 
            \begin{bmatrix}
            Ab_1 & Ab_2 & \cdots & Ab_p
            \end{bmatrix} 
            \]
            where \(b_1, b_2, \cdots, b_p\) are columns of \(B\).
            <br><br>
            So, each column of \(AB\) is a linear combination of the columns of A with weights from the corresponding 
            column of \(B\). Why is not matrix multiplication like just entrywise computation such as sums and scalar 
            multiples of matrices. The key idea is a linear transformation. 
            <strong>The matrix multiplication is "composition" of linear transformations.</strong>
            Let's say a vector \(x\) is multiplied by a matrix \(B\), which means it maps \(x\) into the new vector
            \(Bx\). Next, a matrix \(A\) multiplies the vector \(Bx\). Then we get the resulting vector \(A(Bx)\). 
            Since conposition of "functions" is associative, 
            \[(AB)x = A(Bx)\]
            which allows us to calculate \(Bx\) (matrix x vector) first, instead of \(AB\) (matrix x matrix). 
            This is significant in the case huge matrix computation is required. 
                 
                

        </blockquote> 
        <h1></h1>
        <blockquote>
            
        </blockquote>
        <h1></h1>
        <blockquote>
            
        </blockquote>



    </body>
</html>