<!DOCTYPE html>
<html>
    <head> 
        <title>Covariance</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://cdn.jsdelivr.net/pyodide/v0.23.3/full/pyodide.js"></script>
        <script defer src="https://pyscript.net/alpha/pyscript.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Covariance Matrix</h1>
        <blockquote>
            The <strong>covariance</strong> between two random variables \(X\) and \(Y\) is defined as 
            \[
             \text{Cov }[X, Y] = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])].
            \]
            Here, \((X - \mathbb{E}[X])\) and \((Y - \mathbb{E}[Y])\) are <strong>mean deviations</strong> which represent 
            how far the random variables \(X\) and \(Y\) deviate from their respective expected values(means). So, the covariance 
            measures how these deviation of random variables vary together(<strong>joint variation</strong>).
            <br><br>
            In practice, often we extend this idea to \(n\) random variables. Consider a vector \(x \in \mathbb{R}^n\) whose each 
            entries represent random variables \(X_1, \cdots X_n\).
            <br>
            The <strong>covariance matrix </strong> of the vector \(x\) is defined as
            \[
            \begin{align*}
            \text{Cov }[x] &= \mathbb{E}[(x - \mathbb{E}[x])(x - \mathbb{E}[x])^T]  \\\\
                           &= \begin{bmatrix}
                             \text{Var }[X_1] & \text{Cov }[X_1, X_2] & \cdots & \text{Cov }[X_1, X_n] \\
                             \text{Cov }[X_2, X_1] &  \text{Var }[X_2] & \cdots & \text{Cov }[X_2, X_n] \\
                             \vdots  & \vdots & \ddots & \vdots \\
                             \text{Cov }[X_n, X_1] &   \text{Cov }[X_n, X_2] & \cdots & \text{Var }[X_n]
                             \end{bmatrix}\\\\
                           &= \Sigma
                          
            \end{align*}
            \]
            Each diagonal entry \(\Sigma_{ii}\) represents the <strong>variance</strong> of \(X_i\) because
            \[
            \begin{align*}
            \text{Cov }[X_i, X_i] &= \mathbb{E}[(X_i - \mathbb{E}[X_i])^2]  = \sigma^2\\\\
                                  &= \mathbb{E}[(X_i)^2] - (\mathbb{E}[X_i])^2  \\\\
            \end{align*}
            \]
            Also, the <strong>total variance</strong> of \(\Sigma \) is defined as the trace of \(\Sigma \):
            \[
            \text{tr } (\Sigma ) = \sum_{i=1}^n \text{Var }[X_i].
            \]
            <br>
            The covariance matrix is <strong>symmetric</strong> because the covariance itself is symmetric:
            \[
            \Sigma_{ij} = \text{Cov }[X_i, X_j] = \text{Cov }[X_j, X_i] = \Sigma_{ji}.
            \]
            (Or, for any matrix \(A\), \(\, AA^T\) is symmetric because \((AA^T)^T = (A^T)^TA^T = AA^T \).)
            <br><br>
            Since \(\Sigma\) is symmetric, it is always <strong>orthogonally diagonalizable</strong>(See <a href="../Linear_algebra/symmetry.html">Section I - Part 9</a>): 
            \[
            \Sigma = P D P^T  
            \]
            where \(P\) is an orthogonal matrix whose columns are unit eigenvectors of \(\Sigma\) and \(D\) is a diagonal matrix 
            whose diagonal entries are eigenvalues of \(\Sigma\) corresponding to its eigenvectors. 
            <br><br>
            Note: The total variance of \(\Sigma\) is equal to the sum of its eigenvalues.(ONLY the total!)
            \[
            \text{tr } (\Sigma ) = \sum_{i=1}^n \text{Var }[X_i] = \text{tr } (D) = \sum_{i=1}^n \lambda_i
            \]
            <br><br>
            Moreover, the covariance matrix is always <strong>positive semi-definite</strong>:
            <br>
            For any vector \(v \in \mathbb{R}^n\), 
            \[
            \begin{align*}
            v^T \Sigma v &=  v^T\mathbb{E}[(x - \mathbb{E}[x])(x - \mathbb{E}[x])^T] v \\\\
                         &=  \mathbb{E}[v^T(x - \mathbb{E}[x])(x - \mathbb{E}[x])^T v] \\\\
                         &=  \mathbb{E}[(v^T(x - \mathbb{E}[x]))^2] \geq 0
            \end{align*}
            \]
            Thus, the diagonal entries of \(D\) are <strong>non-negative</strong>. In other words, the eigenvalues of \(\, \Sigma\) 
            always satisfies: 
            \[
            \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0.
            \]
        </blockquote>

        <h1>Principal Component Analysis (PCA)</h1>
        <blockquote>
            The orthogonal diagonalization of the covariance matrix \(\Sigma\) is important in statistics and machine learning for 
            analyzing data and reducing dimensionality:  
            \[
            \Sigma = P D P^T  
            \]
            Here, the column vectors of \(P\) (unit eigenvectors of \(\Sigma \)) are called the <strong>principal components(PCs)</strong> 
            of the data, and the diagonal entries of \(D\) (eigenvalues of \(\Sigma \)) represent the <strong>variances</strong> along these 
            principal components. Each eigenvalue indicates "how much" of the <strong>total variance</strong> of \(\Sigma \) is captured by 
            its corresponding principal component. 
            <br><br>
            The <strong>first principal component(PC1)</strong> corresponds to the largest eigenvalue and represents the direction in the 
            data space where the distribution varies the most(= the direction of maximizing the variance in data). The second principal 
            component(PC2) captures the next largest variance and is orthogonal to PC1. Similarly, each subsequent PC capturing less variance 
            and maintaining orthogonality to all previous PCs.
            <br><br>
            For example, Suppose you have 1 million data points, each represented as a vector \(x_i \in \mathbb{R}^{10} \). 
            After computing the covariance matrix \(\Sigma = X^T X \) and diagonalizing it(\(\Sigma = PDP^T\)), you examine the 
            eigenvalues to determine how much variance each principal component captures comparing with the <strong>total variance</strong> 
            of \(\Sigma\). Let's say PC1: 40%, PC2: 30%, and PC3: 25%. Then, you project the data onto the subspace 
            spanned by the first 3 most significant principal components. Even you discarded the remaining 7 dimentions(noise dimensions),
            you still retain the most significant patterns(trends) in the data. 
            \[
             x_i \in \mathbb{R}^10 \to z_i \in \mathbb{R}^3
            \]
            The vector \(z_i\) is known as the <strong>latent vector</strong>.
            <br><br>
            This <strong>dimensionality reduction</strong> process is called <strong>principal component analysis(PCA)</strong>. By reducing 
            dimensions, PCA enables efficient analysis of large datasets while retaining their most meaningful structure. PCA is 
            widely used in machine learning, image processing, and exploratory data analysis for dimensionality reduction and noise filtering. 
        </blockquote>

        <h1>Singular Value Decomposition(SVD) with PCA</h1>
        <blockquote>
            In practice, the <strong>singular value decomposition</strong>(See <a href="../Linear_algebra/symmetry.html">Section I - Part 9</a>) plays an 
            important role in the PCA. 
            Given a data matrix in mean-deviation form \(X \in \mathbb{R}^{m \times n}\), then \(S = X^TX\) is the covariance matrix.
            <br>
            Note: in PCA, it is common to use \(X^TX \mathbb{R}^{n \times n}\) instead of \(XX^T \mathbb{R}^{m \times m}\) because we are interested 
            in  \(n\) "features" than \(m\) data points. 
            <br>
            By SVD of \(X\), 
            \[
            \begin{align*}
            S &= (U\Sigma V^T)^T(U\Sigma V^T)  \\\\
              &= V\Sigma^TU^TU\Sigma V^T \\\\
              &= V\Sigma^2V^T
            \end{align*}
            \] 
            This result is equivalent to the eigendecomposition \(X^TX = PDP^T\) because the singular value \(\sigma_i\) is the square roots of 
            eigenvalues of the covariance matrix \(S\): 
            [\
            \sigma_i = \sqrt{\lambda_i}
            \]
            and the right singular vectors in \(V\) are eigenvectors(principal components) of \(S\). So, singular values represent 
            <strong>standard deviation</strong> along the principal components.
        <div class="container">
            <pre class="python-code">
                import numpy as np

                # Random data matrix ( m data points with n features)
                def generate_data(m, n):
                    data = np.random.randn(m, n) 

                    # Make some correlations 
                    data[:, 2] = 0.7 * data[:, 0] + 0.5 * data[:, 2]
                    data[:, 3] = 0.2 * data[:, 0] + 0.5 * data[:, 1] 
                    data[:, 4] = -0.3 * data[:, 1] + 0.2 * data[:, 2]
                    data[:, 5] = 0.4 * data[:, 0] + 0.1 * data[:, 1]
                    data[:, 6] = 0.8 * data[:, 3] + -0.3 * data[:, 2]

                    # Mean-deviation form 
                    return (data - np.mean(data, axis=0) )

                # PCA with covariance matrix (As refference)
                def pca_via_covariance(data):
                    
                    # "Sample" covariance matrix 
                    # Note: Dividing by (m-1) provides "unbiased" estimate of the population covariance. 
                    cov_matrix = np.dot(data.T, data) / (data.shape[0] - 1)

                    # Eigenvalue decomposition 
                    # np.linalg.eigh() is for symmetric matrix (real, diagonaizable), better than np.linalg.eig() 
                    eigvals, eigvecs = np.linalg.eigh(cov_matrix) 

                    #  Make eigenvalues & eigenvectors in descending order
                    idx = np.argsort(eigvals)[::-1] 
                    eigvals = eigvals[idx]
                    eigvecs = eigvecs[:, idx] 

                    # Each variance vs total variance
                    ratio = eigvals / np.sum(eigvals)

                    return eigvals, eigvecs, ratio

                # PCA with SVD (we use this function)
                def pca_with_svd(data):
                    
                    # Singular Value Decomposition (we don't need the matrix U: use "_" )
                    _, S, vt = np.linalg.svd(data, full_matrices=False)
                    
                    # Get eigenvalues via singular values: lambda_i = (S_i)^2  / m - 1
                    eigvals = (S ** 2) / (data.shape[0] - 1)
                    
                    # Each variance vs total variance
                    ratio = eigvals / np.sum(eigvals)
                    
                    return eigvals, vt.T, ratio 

                # Set 90% threshold for the number of PCs
                def threshold(var_ratio, t):
                    
                    # Compute cumulative variance
                    cum_variance = np.cumsum(var_ratio)
                    
                    # Find the number of components for 90% variance retention
                    num_pcs = np.argmax(cum_variance >= t) + 1
                    return num_pcs

                # Dimension of data 
                m = 1000000 
                n = 10 

                # Run PCA
                eigvals, eigvecs, ratio = pca_with_svd(generate_data(m, n))

                # Threshold for variance retention
                t = 0.95
                num_pcs = threshold(ratio, t)

                # Print results
                print("Eigenvalues:")
                for i, val in enumerate(eigvals):
                    print(f"  Lambda {i + 1}: {val:.6f}")

                print("\nExplained Variance Ratio (%):")
                for i, var in enumerate(ratio):
                    print(f"  PC{i + 1}: {var * 100:.2f}%")

                print(f"\nTo retain {t * 100:.0f}% of variance, use the first {num_pcs} PCs.")
            </pre>
            <button class="run-button" onclick="runPythonCode()">Run Code</button>
            <div class="python-output" id="output"></div>
        </div>
         </blockquote>
        <a href="../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>    

        <script> 
            let pyodide; 
            async function loadPyodideAndPackages() { 
                pyodide = await loadPyodide(); 
                await pyodide.loadPackage("numpy");
                console.log("Pyodide and numpy loaded successfully."); 
            } 
            async function runPythonCode() { 
                const pythonCode = document.querySelector('.python-code').textContent; 
                const outputContainer = document.getElementById('output'); 
                outputContainer.innerHTML = ""; 
                try { 
                    let capturedOutput = []; 
                    pyodide.globals.set("print", function(...args) { 
                        capturedOutput.push(args.join(" ")); 
                    }); 
                    await pyodide.runPythonAsync(pythonCode); 
                    outputContainer.textContent = capturedOutput.join("\n"); 
                } catch (error) { 
                    outputContainer.textContent = `Error: ${error.message}`; 
                    console.error("Execution error:", error); } 
                } 
                loadPyodideAndPackages(); 
        </script>
    </body>
</html>