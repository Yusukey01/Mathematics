<!DOCTYPE html>
<html>
    <head> 
        <title>Covariance</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Covariance Matrix</h1>
        <blockquote>
            The <strong>covariance</strong> between two random variables \(X\) and \(Y\) is defined as 
            \[
             \text{Cov }[X, Y] = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])].
            \]
            Here, \((X - \mathbb{E}[X])\) and \((Y - \mathbb{E}[Y])\) are <strong>mean deviations</strong> which represent 
            how far the random variables \(X\) and \(Y\) deviate from their respective expected values(means). So, the covariance 
            measures how these deviation of random variables vary together(<strong>joint variation</strong>).
            <br><br>
            In practice, often we extend this idea to \(n\) random variables. Consider a vector \(x \in \mathbb{R}^n\) whose each 
            entries represent random variables \(X_1, \cdots X_n\).
            <br>
            The <strong>covariance matrix </strong> of the vector \(x\) is defined as
            \[
            \begin{align*}
            \text{Cov }[x] &= \mathbb{E}[(x - \mathbb{E}[x])(x - \mathbb{E}[x])^T]  \\\\
                           &= \begin{bmatrix}
                             \text{Var }[X_1] & \text{Cov }[X_1, X_2] & \cdots & \text{Cov }[X_1, X_n] \\
                             \text{Cov }[X_2, X_1] &  \text{Var }[X_2] & \cdots & \text{Cov }[X_2, X_n] \\
                             \vdots  & \vdots & \ddots & \vdots \\
                             \text{Cov }[X_n, X_1] &   \text{Cov }[X_n, X_2] & \cdots & \text{Var }[X_n]
                             \end{bmatrix}\\\\
                           &= \Sigma
                          
            \end{align*}
            \]
            Each diagonal entry \(\Sigma_{ii}\) represents the <strong>variance</strong> of \(X_i\) because
            \[
            \begin{align*}
            \text{Cov }[X_i, X_i] &= \mathbb{E}[(X_i - \mathbb{E}[X_i])^2]  = \sigma^2\\\\
                                  &= \mathbb{E}[(X_i)^2] - (\mathbb{E}[X_i])^2  \\\\
            \end{align*}
            \]
            Also, the <strong>total variance</strong> of \(\Sigma \) is defined as the trace of \(\Sigma \):
            \[
            \text{tr } (\Sigma ) = \sum_{i=1}^n \text{Var }[X_i].
            \]
            <br>
            The covariance matrix is <strong>symmetric</strong> because the covariance itself is symmetric:
            \[
            \Sigma_{ij} = \text{Cov }[X_i, X_j] = \text{Cov }[X_j, X_i] = \Sigma_{ji}.
            \]
            (Or, for any matrix \(A\), \(\, AA^T\) is symmetric because \((AA^T)^T = (A^T)^TA^T = AA^T \).)
            <br><br>
            Since \(\Sigma\) is symmetric, it is always <strong>orthogonally diagonalizable</strong>: 
            \[
            \Sigma = P D P^T  
            \]
            where \(P\) is an orthogonal matrix whose columns are unit eigenvectors of \(\Sigma\) and \(D\) is a diagonal matrix 
            whose diagonal entries are eigenvalues of \(\Sigma\) corresponding to its eigenvectors. 
            <br><br>
            Note: The total variance of \(\Sigma\) is equal to the sum of its eigenvalues.
            \[
            \text{tr } (\Sigma ) = \sum_{i=1}^n \text{Var }[X_i] = \text{tr } (D) = \sum_{i=1}^n \lambda_i
            \]
            <br><br>
            Moreover, the covariance matrix is always <strong>positive semi-definite</strong>:
            <br>
            For any vector \(v \in \mathbb{R}^n\), 
            \[
            \begin{align*}
            v^T \Sigma v &=  v^T\mathbb{E}[(x - \mathbb{E}[x])(x - \mathbb{E}[x])^T] v \\\\
                         &=  \mathbb{E}[v^T(x - \mathbb{E}[x])(x - \mathbb{E}[x])^T v] \\\\
                         &=  \mathbb{E}[(v^T(x - \mathbb{E}[x]))^2] \geq 0
            \end{align*}
            \]
            Thus, the diagonal entries of \(D\) are <strong>non-negative</strong>. In other words, the eigenvalues of \(\, \Sigma\) 
            always satisfies: 
            \[
            \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0.
            \]
        </blockquote>

        <h1>Principal Component Analysis (PCA)</h1>
        <blockquote>
            The orthogonal diagonalization of the covariance matrix \(\Sigma\) is important in statistics and machine learning for 
            analyzing data and reducing dimensionality:  
            \[
            \Sigma = P D P^T  
            \]
            Here, the column vectors of \(P\) (unit eigenvectors of \(\Sigma \)) are called the <strong>principal components(PCs)</strong> 
            of the data, and the diagonal entries of \(D\) (eigenvalues of \(\Sigma \)) represent the <strong>variances</strong> along these 
            principal components. Each eigenvalue indicates "how much" of the <strong>total variance</strong> of \(\Sigma \) is captured by 
            its corresponding principal component. 
            <br><br>
            The <strong>first principal component(PC1)</strong> corresponds to the largest eigenvalue and represents the direction in the 
            data space where the distribution varies the most(= the direction of maximizing the variance in data). The second principal 
            component(PC2) captures the next largest variance and is orthogonal to PC1. Similarly, each subsequent PC capturing less variance 
            and maintaining orthogonality to all previous PCs.
            <br><br>
            For example, Suppose you have 10 million data points, each represented as a vector \(x_i \in \mathbb{R}^{10} \). 
            After computing the covariance matrix \(\Sigma \) and diagonalizing it(\(\Sigma = PDP^T\)), you examine the 
            eigenvalues to determine how much variance each principal component captures comparing with the <strong>total variance</strong> 
            of \(\Sigma\). Let's say PC1: 40%, PC2: 30%, and PC3: 25%. Then, you project the data onto the subspace 
            spanned by the first 3 most significant principal components. Even you discarded the remaining 7 dimentions(noise dimensions),
            you still retain the most significant patterns(trends) in the data. 
            <br><br>
            This <strong>dimensionality reduction</strong> process is called <strong>principal component analysis(PCA)</strong>. By reducing 
            dimensions, PCA enables efficient analysis of large datasets while retaining their most meaningful structure. PCA is 
            widely used in machine learning, image processing, and exploratory data analysis for dimensionality reduction and noise filtering. 
        </blockquote>

        <a href="../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>    
    </body>
</html>