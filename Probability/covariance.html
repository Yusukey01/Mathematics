<!DOCTYPE html>
<html>
    <head> 
        <title>Covariance</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Covariance</h1>
        <blockquote>
            The <strong>covariance</strong> between two random variables \(X\) and \(Y\) is defined as 
            \[
             \text{Cov }[X, Y] = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])].
            \]
            Here, \((X - \mathbb{E}[X])\) and \((Y - \mathbb{E}[Y])\) are <strong>mean deviations</strong> which represent 
            how far the random variables \(X\) and \(Y\) deviate from their respective expected values(means). So, the covariance 
            measures how these deviation of random variables vary together(<strong>joint variation</strong>).
            <br><br>
            In practice, often we extend this idea to \(n\) random variables. 
            <br>
            Consider a vector \(x \in \mathbb{R}^n\) whose each entries represent random variables \(X_1, \cdots X_n\).
            The <strong>covariance matrix </strong> of the vector \(x\) is defined as
            \[
            \begin{align*}
            \text{Cov }[x] &= \mathbb{E}[(x - \mathbb{E}[x])(x - \mathbb{E}[x])^T]  \\\\
                           &= \begin{bmatrix}
                             \text{Var }[X_1] & \text{Cov }[X_1, X_2] & \cdots & \text{Cov }[X_1, X_n] \\
                             \text{Cov }[X_2, X_1] &  \text{Var }[X_2] & \cdots & \text{Cov }[X_2, X_n] \\
                             \vdots  & \vdots & \ddots & \vdots \\
                             \text{Cov }[X_n, X_1] &   \text{Cov }[X_n, X_2] & \cdots & \text{Var }[X_n]
                             \end{bmatrix}\\\\
                           &= \Sigma
                          
            \end{align*}
            \]
            Each diagonal entry \(\Sigma_{ii}\) represents the <strong>variance</strong> of \(X_i\) because
            \[
            \begin{align*}
            \text{Cov }[X_i, X_i] &= \mathbb{E}[(X_i - \mathbb{E}[X_i])^2]  = \sigma^2\\\\
                                  &= \mathbb{E}[(X_i)^2] -2 (\mathbb{E}[X_i])^2 + (\mathbb{E}[X_i])^2 \\\\
                                  &= \mathbb{E}[(X_i)^2] - (\mathbb{E}[X_i])^2  \\\\
            \end{align*}
            \]
            Also, the <strong>total variance</strong> of \(\Sigma \) is defined as 
            \[
            \text{tr } \Sigma = \sum{i=1}^n \text{Var }[X_i].
            \]
            <br>
            The covariance matrix (\Sigma\) is always <strong>symmetric</strong> because the covariance itself is symmetric:
            \[
            \text{Cov }[X_i, X_j] = \text{Cov }[X_j, X_i].
            \]
            Or, simply said, for any matrix \(A\), always \(AA^T\) is symmetric:
            \[
            (AA^T)^T = (A^T)^TA^T = AA^T.
            \]
            So, since \(\Sigma\) is symmetric, it is always <strong>orthogonally diagonalizable</strong>: 
            \[
            \Sigma = P D P^T  
            \]
            where \(P\) is an orthogonal matrix whose columns ar eigenvectors of \(\Sigma\) and \(D\) is a diagonal matrix 
            whose diagonal entries are eigenvalues of \(\Sigma\) corresponding to its eigenvectors. 
            <br><br>
            Moreover, the covariance matrix \(\Sigma \) is always <strong>positive semi-definite</strong>:
            <br>
            For any vector \(v \in \mathbb{R}^n\), 
            \[
            \begin{align*}
            v^T \Sigma v &=  v^T\mathbb{E}[(x - \mathbb{E}[x])(x - \mathbb{E}[x])^T] v \\\\
                         &=  \mathbb{E}[v^T(x - \mathbb{E}[x])(x - \mathbb{E}[x])^T v] \\\\
                         &=  \mathbb{E}[(v^T(x - \mathbb{E}[x]))^2] \geq 0
            \end{align*}
            \]
            Thus, the diagonal entries of \(D\) are <strong>non-negative</strong>. In other words, the eigenvalues of \(\, \Sigma\) 
            always satisfies: 
            \[
            \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0.
            \]
            <br>
            The column vectors of \(P\) (eigenvectors of \(\Sigma \)) are called the <strong>principal components</strong> of the data, and 
            the diagonal entries of \(D\) (<strong>eigenvalues</strong> of \(\Sigma \)) represent the <strong>variances</strong> along these 
            principal components, which indicate "how much" of the <strong>total variance</strong> of \(\Sigma \) is captured by each principal 
            component.
        </blockquote>
        <a href="../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>

        
    </body>
</html>