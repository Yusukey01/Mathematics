<!DOCTYPE html>
<html>
    <head> 
        <title>Covariance</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://cdn.jsdelivr.net/pyodide/v0.23.3/full/pyodide.js"></script>
        <script defer src="https://pyscript.net/alpha/pyscript.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Cross-covariance</h1>
        <blockquote>
            Often we want to handle multiple datasets or different sets of variables that are related but might 
            not be directly comparable in terms of the data structures. In such a case, <strong>cross-covariance</strong> 
            measures the covariance between different pairs of variables from different datasets.
            <br><br>
            For example, consider two different datasets \(A \in \mathbb{R}^{m \times n_1}\) and \(B \in \mathbb{R}^{m \times n_2}\). 
            We can compute the <strong>cross-covariance matrix</strong>:
            \[
            C_{AB} = \frac{1}{m-1}A^TB
            \]
            (Both dataset \(A\) and \(B\) must have the same number of data points.)
            <br>
            Note: C_{AA} is called the <strong>auto-covariance matrix</strong>, which is the same as the covariance matrix that 
            we discussed in Part 1. 
        </blockquote>

        <h1>Correlation</h1>
        <blockquote>
            The magnitude of the covariance between two random variables \(X\) and \(Y\) depends on the units of the variables, we 
            <strong>standardize</strong> the covariance by the standard deviation of the variables: 
            \[
            \sigma_X  = \mathbb{E}[(X - \mathbb{E}[X])^2]  \text{ and } \sigma_Y  = \mathbb{E}[(Y - \mathbb{E}[Y])^2]
            \]
            Then we obtain the <strong>correlation coefficient</strong> between \(X\) and \(Y\) quantifies the strength 
            and direction of their <strong>linear</strong> relationship: 
            \[
            \rho = \text{corr }[X, Y] = \frac{\text{Cov }[X, Y]}{\sigma_X \sigma_Y}.
            \]
            Now it it easy to compare across different datasets.
            <br><br>
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Boundedness of Correlation Coefficient</span> 
                \[
                -1 \leq \rho \leq 1
                \]
                Note: 
                <br>
                \(\rho = 1\) indicates a perfect positive linear relationship.
                <br>
                \(\rho = -1\) indicates a perfect negative linear relationship.
                <br>
                \(\rho = 0\) indicates no <strong>linear</strong> relationship. 
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                We use the Cauchy-Schwarz inequality for random variables:
                \[
                \mathbb{E}[XY]^2 \leq  \mathbb{E}[X]^2 \mathbb{E}[Y]^2
                \]
                Note: \(\mathbb{E}[XY]\) is the inner product on the set of random variables \(X\) and \(Y\).
                <br><br>
                Substitue standardized variables
                \[
                \mathbb{E}[\frac{(X - \mathbb{E}[X])}{\sigma_X}\cdot \frac{(Y - \mathbb{E}[Y])}{\sigma_Y}]^2 
                \leq  \mathbb{E}[\frac{(X - \mathbb{E}[X])}{\sigma_X}]^2 \, \mathbb{E}[\frac{(Y - \mathbb{E}[Y])}{\sigma_Y}]^2
                \]
            </div>
        </blockquote>




        <a href="../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>   
    </body>
</html>