<!DOCTYPE html>
<html>
    <head> 
        <title>Covariance</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://cdn.jsdelivr.net/pyodide/v0.23.3/full/pyodide.js"></script>
        <script defer src="https://pyscript.net/alpha/pyscript.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Cross-covariance</h1>
        <blockquote>
            Often we want to handle multiple datasets or different sets of variables that are related but might 
            not be directly comparable in terms of the data structures. In such a case, the <strong>cross-covariance</strong> 
            measures the covariance between different pairs of variables from different datasets.
            <br><br>
            For example, consider two different datasets \(A \in \mathbb{R}^{m \times n_1}\) and \(B \in \mathbb{R}^{m \times n_2}\). 
            We can compute the <strong>sample cross-covariance matrix</strong>:
            \[
            K_{AB} = \frac{1}{m-1}(A-\bar{A})^T(B-\bar{B})
            \]
            (Both \(A\) and \(B\) must have the same number of data points.)
            <br>
            Note: \(K_{AA}\) is called the <strong>auto-covariance matrix</strong>, which is the same as the covariance matrix that 
            we discussed in Part 5. 
        </blockquote>

        <h1>Correlation</h1>
        <blockquote>
            The magnitude of the covariance between two random variables \(X\) and \(Y\) depends on the units of the variables, we 
            <strong>standardize</strong> the covariance by the standard deviation of the variables: 
            \[
            \sigma_X  = \sqrt{\mathbb{E}[(X - \mu_X])^2]}  \text{ and } \sigma_Y  = \sqrt{\mathbb{E}[(Y - \mu_Y)^2]}
            \]
            Then we obtain the <strong> population correlation coefficient</strong> between \(X\) and \(Y\) quantifies the strength 
            and direction of their <strong>linear</strong> relationship \(Y = aX +b\): 
            \[
            \begin{align*}
            \rho_{X, Y} = \text{Corr }[X, Y] &= \frac{\text{Cov }[X, Y]}{\sigma_X \sigma_Y} \\\\
                                             &= \frac{\mathbb{E }[(X - \mu_X)(Y-\mu_Y)]}{\sigma_X \sigma_Y}.
            \end{align*}
            \]
            Also, the <strong>sample correlation coefficient</strong> is defined as:
            \[
            r_{xy} = \frac{1}{(n-1)s_x s_y}\sum_{i =1}^n (x_i - \bar{x})(y_i - \bar{y})
            \]
            where \(\bar{x},\, \bar{y}\) are sample means and \(s_x \, , s_y\) are corrected sample standard deviations.
            <br>
            Note: the <strong>corrected sample standard deviation</strong> is an unbiased estimator of the population standard deviation:  
            \[
            s_x = \sqrt{\frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2}.
            \]
            <br><br>
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Boundedness of Correlation Coefficient</span> 
                \[
                -1 \leq \rho \leq 1
                \]
                Note: 
                <br>
                \(\rho = 1\) indicates a perfect positive linear relationship.
                <br>
                \(\rho = -1\) indicates a perfect negative linear relationship.
                <br>
                \(\rho = 0\) indicates no <strong>linear</strong> relationship. 
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                We use the Cauchy-Schwarz inequality for random variables:
                \[
                \mathbb{E}[XY]^2 \leq  \mathbb{E}[X]^2 \mathbb{E}[Y]^2
                \]
                Note: \(\mathbb{E}[XY]\) is the inner product on the set of random variables \(X\) and \(Y\).
                <br><br>
                Substitue standardized variables
                \[
                \begin{align*}
                & \mathbb{E}[\frac{(X - \mathbb{E}[X])}{\sigma_X}\cdot \frac{(Y - \mathbb{E}[Y])}{\sigma_Y}]^2 
                  \leq  \mathbb{E}[\frac{(X - \mathbb{E}[X])}{\sigma_X}]^2 \, \mathbb{E}[\frac{(Y - \mathbb{E}[Y])}{\sigma_Y}]^2 \\\\
                &\Longrightarrow  \rho^2 \leq 1 \cdot 1 \\\\
                &\Longrightarrow  -1 \leq \rho \leq 1
                \end{align*}
                \]
            </div>
            The <strong>correlation matrix</strong> is essentially a standardized covariance matrix. It is often used in PCA when 
            features have significantly different scales to ensure fair weighting. The correlation matrix of the vector \(x \in \mathbb{R}^n\) is defined as
            \[
            \begin{align*}
            R &= \text{Corr }[x] \\\\
                           &= \begin{bmatrix}
                             1 & \text{Corr }[X_1, X_2] & \cdots & \text{Corr }[X_1, X_n] \\
                             \text{Corr }[X_2, X_1] &  1 & \cdots & \text{Corr }[X_2, X_n] \\
                             \vdots  & \vdots & \ddots & \vdots \\
                             \text{Corr }[X_n, X_1] &   \text{Corr }[X_n, X_2] & \cdots & 1
                             \end{bmatrix} \\\\
              &= (\text{diag }(K_{xx}))^{-\frac{1}{2}}K_{xx}(\text{diag }(K_{xx}))^{-\frac{1}{2}}
            \end{align*}
            \]
           where \(K_{xx}\) is the auto-covariance matrix: 
           \[
           K_{xx} = \mathbb{E }[(x - \mu)(x - \mu)^T] = R_{xx} - \mu \mu^T
           \]
           where \(R_{xx} = \mathbb{E }[xx^T]\) is the auto-correlation matrix. 
        </blockquote>




        <a href="../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>   
    </body>
</html>