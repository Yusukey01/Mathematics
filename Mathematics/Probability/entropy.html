<!DOCTYPE html>
<html>
    <head> 
        <title>Linear Regression</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Entropy</h1>
        <blockquote>
            Consider you play a number guessing game. You have to guess a natural number between 1 and 100. If I tell you that the number is not 1, then 
            you still have 99 possible choices. So, this information is not valuable. In this case, we can say that the <strong>information content</strong> 
            of this clue is low. However, if I tell you that the number is divisible by 11, the number of choices is reduced to 9 only and thus the information 
            content of this clue is much higher. 
            <br><br>
            This example illustrates that information content (or <strong>self-information</strong>) measures the "uncertainty" associated with an 
            event. The lower the probability of an event, the higher its information content.
            <br><br>
            We extend this concept by quantifying the average information content across all possible outcomes of a 
            random variable \(X\).
            <br>
            The <strong>entropy</strong> of a discrete random variable \(X\) with distribution \(p\) over 
            \(K\) states is defined as:
            \[
            \mathbb{H}(X) = - \sum_{k=1}^K p(X = k) \log_2 p(X = k) = - \mathbb{E }_X [\log_2 p(X)]
            \]
            Note: The choice of logarithmic base determines the units of entropy. In this case, the units is <strong>bits</strong>. 
            Also, often we use the natural logarithm (base \(e\))with the units <strong>nats</strong>.
        </blockquote>

        <h1>Joint Entropy</h1>
        <blockquote>
            The <strong>joint entropy</strong> between two random variables \(X\) and \(Y\) is defined as
            \[
            \mathbb{H}(X, Y) = - \sum_{x, y} p(x, y) \log_2 p(x, y).
            \]
            Note:
            \[
            \mathbb{H}(X) + \mathbb{H}(Y) \geq \mathbb{H}(X, Y) \geq \max\{\mathbb{H}(X), \mathbb{H}(Y)\} \geq 0
            \]
        </blockquote>

        <h1>Conditional Entropy</h1>
        <blockquote>
            The <strong>conditional entropy</strong> of \(Y\) given \(X\) is the uncertainty we have in \(Y\) after 
            seeing \(X\), averaged over possible values for \(X\). 
            \[
            \mathbb{H}(Y | X) = - \mathbb{E }_{p(X)}[ \mathbb{H}(p(Y | X))]
            \]
        </blockquote>

        <h1>Cross Entropy</h1>
        <blockquote>
            The <strong>cross entropy</strong> between two "distributions" \(p\) and \(q\) is defined as
            \[
            \mathbb{H}(p, q) = - \sum_{k=1}^K p_k \log  q_k.
            \]
        </blockquote>

        <br><a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>   
    </body>
</html>