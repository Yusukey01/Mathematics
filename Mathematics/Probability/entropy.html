<!DOCTYPE html>
<html>
    <head> 
        <title>Linear Regression</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Entropy</h1>
        <blockquote>
            Consider you play a number guessing game. You have to guess a natural number between 1 and 100. If I tell you that the number is not 1, then 
            you still have 99 possible choices. So, this information is not valuable. In this case, we can say that the <strong>information content</strong> 
            of this clue is low. However, if I tell you that the number is divisible by 11, the number of choices is reduced to 9 only and thus the information 
            content of this clue is much higher. 
            <br><br>
            This example illustrates that information content (or <strong>self-information</strong>) measures the "uncertainty" associated with an 
            event. The lower the probability of an event \(P(E)\), the higher its information content, \(I(E)\). So, we can define the information content as follows:
            \[
            I(E) = - \log P(E) \geq 0.
            \]
            Note: If two events \(A\) and \(B\) are independent, the information content is <strong>additive</strong>:  
            \[
            I(A, B) = - \log(P(A)\cdot P(B)) = - (\log P(A) + \log P(B)) = I(A) + I(B).
            \]
            This is the main reason why we use the logarithm. We can convert the multiplication of probabilities into the 
            addition of information contents. 
            <br><br>
            We extend this concept by quantifying the "expected value" of the information content across all possible outcomes of a 
            discrete random variable \(X\).
            <br>
            The <strong>entropy</strong> of a discrete random variable \(X\) with distribution \(P\) over 
            \(n\) states is defined as:
            \[
            \mathbb{H}(X) = - \sum_{k=1}^n P(X = k) \log_2 P(X = k) = \mathbb{E }_X [- \log_2 P(X)]
            \]
            Note: The choice of logarithmic base determines the units of entropy. In this case, the units is <strong>bits</strong>. 
            Also, often we use the natural logarithm (base \(e\))with the units <strong>nats</strong>.
        </blockquote>

        <h1>Joint Entropy</h1>
        <blockquote>
            The <strong>joint entropy</strong> between two random variables \(X\) and \(Y\) is defined as
            \[
            \mathbb{H}(X, Y) = - \sum_{x, y} P(x, y) \log P(x, y).
            \]
            If \(X\) and \(Y\) are independent, \(\mathbb{H}(X, Y) =  \mathbb{H}(X) + \mathbb{H}(Y)\). Otherwise, the sum is 
            larger than \(\mathbb{H}(X, Y)\). Also, if \(Y\) is a deterministic function of \(X\), then \(\mathbb{H}(X, Y) = \mathbb{H}(X)\). Thus, 
            \[
            \mathbb{H}(X) + \mathbb{H}(Y) \geq \mathbb{H}(X, Y) \geq \max\{\mathbb{H}(X), \mathbb{H}(Y)\} \geq 0.
            \]
            This is true for more than two random variables. 
        </blockquote>

        <h1>Conditional Entropy</h1>
        <blockquote>
            The <strong>conditional entropy</strong> of \(Y\) given \(X\) is the uncertainty we have in \(Y\) after 
            seeing \(X\), averaged over possible values for \(X\):
            \[
            \mathbb{H}(Y | X) = \mathbb{E }_{P(X)}[ \mathbb{H}(P(Y | X))]
            \]
            <div class="proof">
                <span class="proof-title"> \(\mathbb{H}(Y | X)  = \mathbb{H}(X, Y) -  \mathbb{H}(X)\):</span>
                \[
                \begin{align*}
                \mathbb{H}(Y | X) &=  \mathbb{E }_{P(X)}[ \mathbb{H}(P(Y | X))] \\\\
                                  &=  \sum_{x} P(x) \mathbb{H}(P(Y | X = x)) \\\\
                                  &= - \sum_{x} P(x) \sum_{y} P(y | x) \log P(y | x) \\\\\
                                  &= - \sum_{x, y} P(x, y) \log P(y | x) \\\\
                                  &= - \sum_{x, y} P(x, y) \log \frac{P(x,y)}{P(x)}\\\\
                                  &= - \sum_{x, y} P(x, y) \log P(x,y) + \sum_{x} P(x) \log P(x) \\\\
                                  &= \mathbb{H}(X, Y) -  \mathbb{H}(X) \tag{1}
                \end{align*}
                \]
            </div>
            Note: If \(Y\) can be completely determined by \(X\), \(\mathbb{H}(Y | X)  = 0\). 
            Also, if both \(X\) and \(Y\) are independent each other,  \(\mathbb{H}(Y | X)  =  \mathbb{H}(Y) \).
            <br><br>
            By (1), \(\mathbb{H}(X, Y) = \mathbb{H}(X) + \mathbb{H}(Y | X) \). This implies the <strong>chain rule</strong> of entropy: 
            \[
            \mathbb{H}(X_1, X_2, \cdots, X_n) = \sum_{i =1}^n \mathbb{H}(X_i |X_1, \cdots,  X_{i-1}).
            \]
            
        </blockquote>

        <h1>Cross Entropy</h1>
        <blockquote>
            The <strong>cross entropy</strong> of a distribution \(q\) relative to a distribution \(p\) is defined as
            \[
            \mathbb{H}(p, q) = - \sum_{k=1}^n p_k \log  q_k.
            \]
        </blockquote>

        <h1>KL Divergence (Relative Entropy, Information Gain)</h1>
        <blockquote>
            It is important to define a distance metric to measure "how similar" two distributions \(p\) and \(q\) are. However, 
            instead, we can define a <strong>divergence measure</strong> \(D(p,q)\), which only requires \(D(p, q) \geq 0\) with 
            equality if and only if \(p = q\). Here, we introduce the <strong>Kullback-Leibler divergence</strong> between two 
            distributions \(p\) and \(q\): 
            \[
            \begin{align*}
            D_{\mathbb{KL}}(p \| q) &= \sum_{k=1}^n p_k \log \frac{p_k}{q_k} \\\\
                                   &= \sum_{k=1}^n p_k \log p_k - \sum_{k=1}^n p_k \log q_k \\\\
                                   &= - \mathbb{H}(p) + \mathbb{H}(p, q) \tag{2}
            \end{align*}
            \]
            where \(p\) and \(q\) are defined on the same sample space \(\mathcal{X}\).
            <br>
            For example, \(D_{\mathbb{KL}}(p \| q)\) can measure how much an estimated distribution \(q\) is different from 
            a true distribution \(p\). If the estimation of \(p\) by \(q\) is "good," the KL divergence will be close to zero. 
            <br><br>
            By the expression (2), the cross entropy can be written as:
            \[
            \mathbb{H}(p, q) = \mathbb{H}(p) +  D_{\mathbb{KL}}(p \| q).
            \]
            So, if \(p(x)\) is fixed, minimizing the cross entropy is equivalent to minimizing KL divergence. 
            <br><br>

            <div class="theorem">
                <span class="theorem-title">Theorem 1: Non-negativity of KL Divergence</span>
                \[
                D_{\mathbb{KL}}(p \| q)  \geq 0
                \]
                where \(p\) and \(q\) are discrete distributions defined on the same sample space \(\mathcal{X}\) and with 
                equality if and only if \(p = q\).
                <br>
                Or, the entropy of \(p\) is less than or equal to its cross entropy with any other distribution \(q\):
                \[
                \begin{align*}
                &\sum_{k=1}^n p_k \log p_k - \sum_{k=1}^n p_k \log q_k \geq 0 \\\\
                &\Longrightarrow  - \sum_{k=1}^n p_k \log p_k \leq  - \sum_{k=1}^n p_k \log q_k.
                \end{align*}
                \]
                This is called <strong>Gibbs' inequality</strong>.
                
            </div>
            There are many ways to prove this theorem, here we introduce <strong>log sum inequality</strong> to prove Theorem 1.
            <div class="theorem">
                <span class="theorem-title">Theorem 2: Log Sum Inequality</span>
                For nonnegative numbers \(a_1, a_2, \cdots, a_n\) and \(b_1, b_2, \cdots, b_n\), 
                \[
                \sum_{k=1}^n a_k \log \frac{a_k}{b_k} \geq \Big(\sum_{k=1}^n a_k \Big) \log \frac{\sum_{k=1}^n a_k}{\sum_{k=1}^n b_k} \tag{3}
                \]
                with equality if and only if \(\frac{a_k}{b_k} = constant\).
            </div>

            <div class="proof">
                <span class="proof-title">Proof of theorem 2:</span>
                Assume without loss of generality that \(a_k >0\) and \(b_k >0\). Consider a function \(f(x) = x \log x\). Since 
                \(f'(x) = \log x + 1\) and  \(f''(x) = \frac{1}{x}\), the function is a concave function for all \(x > 0\).
                <br>
                Let \(\lambda_i = \frac{b_k}{\sum_{k=1}^n b_k}\), then the left side of inequality (3) becomes
                \[
                \begin{align*}
                \sum_{k=1}^n a_k \log \frac{a_k}{b_k}
                &= \sum_{k=1}^n b_k \frac{a_k}{b_k} \log \frac{a_k}{b_k} \\\\
                &= \sum_{k=1}^n b_k \frac{\sum_{k=1}^n b_k}{\sum_{k=1}^n b_k}f(\frac{a_k}{b_k}) \\\\
                &= \Big(\sum_{k=1}^n b_k \Big) \sum_{k=1}^n \lambda_k f(\frac{a_k}{b_k})\\\\
                \end{align*}
                \]
                Here, by <strong>Jensen's inequality</strong>,
                \[
                \begin{align*}
                \Big(\sum_{k=1}^n b_k \Big) \sum_{k=1}^n \lambda_k f(\frac{a_k}{b_k})
                                                     &\geq \Big(\sum_{k=1}^n b_k \Big) f\Big(\sum_{k=1}^n \lambda_k \frac{a_k}{b_k} \Big) \\\\
                                                      &= \Big(\sum_{k=1}^n b_k \Big) f\Big( \frac{\sum_{k=1}^n a_k}{\sum_{k=1}^n b_k} \Big) \\\\
                                                      &=  \Big(\sum_{k=1}^n a_k \Big) \log \frac{\sum_{k=1}^n a_k}{\sum_{k=1}^n b_k}
                \end{align*}
                \]
            </div>

            <div class="proof">
                <span class="proof-title">Proof of theorem 1:</span>
                Suppose \(\sum_{k=1}^n p_k = \sum_{k=1}^n q_k = 1\) and \(p_k, \, q_k \geq 0\).
                By the definition of LK divergence, 
                \[
                D_{\mathbb{KL}}(p \| q) = \sum_{k=1}^n p_k \log \frac{p_k}{q_k}.
                \]
                Using Log sum inequality with \(a_k = p_k\) and \(b_k = q_k\), 
                \[
                \sum_{k=1}^n p_k \log \frac{p_k}{q_k} \geq \Big(\sum_{k=1}^n p_k \Big) \log \frac{\sum_{k=1}^n p_k}{\sum_{k=1}^n q_k}.
                \]
                Since \(\sum_{k=1}^n p_k = \sum_{k=1}^n q_k = 1\), 
                \[
                \sum_{k=1}^n p_k \log \frac{p_k}{q_k} \geq \Big(\sum_{k=1}^n p_k \Big) \log 1 = 0
                \]
                with equality if and only if \(\forall k, \, p_k = q_k\).
            </div>
            
        </blockquote>

        <br><a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>   
    </body>
</html>