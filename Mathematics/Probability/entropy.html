<!DOCTYPE html>
<html>
    <head> 
        <title>Linear Regression</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Entropy</h1>
        <blockquote>
            Consider you play a number guessing game. You have to guess a natural number between 1 and 100. If I tell you that the number is not 1, then 
            you still have 99 possible choices. So, this information is not valuable. In this case, we can say that the <strong>information content</strong> 
            of this clue is low. However, if I tell you that the number is divisible by 11, the number of choices is reduced to 9 only and thus the information 
            content of this clue is much higher. 
            <br><br>
            This example illustrates that information content (or <strong>self-information</strong>) measures the "uncertainty" associated with an 
            event. The lower the probability of an event \(P(E)\), the higher its information content, \(I(E)\). So, we can define the information content as follows:
            \[
            I(E) = - \log P(E) \geq 0.
            \]
            Note: If two events \(A\) and \(B\) are independent, the information content is <strong>additive</strong>:  
            \[
            I(A, B) = - \log(P(A)\cdot P(B)) = - (\log P(A) + \log P(B)) = I(A) + I(B).
            \]
            This is the main reason why we use the logarithm. We can convert the multiplication of probabilities into the 
            addition of information contents. 
            <br><br>
            We extend this concept by quantifying the "expected value" of the information content across all possible outcomes of a 
            discrete random variable \(X\).
            <br>
            The <strong>entropy</strong> of a discrete random variable \(X\) with distribution \(P\) over 
            \(K\) states is defined as:
            \[
            \mathbb{H}(X) = - \sum_{k=1}^K P(X = k) \log_2 P(X = k) = \mathbb{E }_X [- \log_2 P(X)]
            \]
            Note: The choice of logarithmic base determines the units of entropy. In this case, the units is <strong>bits</strong>. 
            Also, often we use the natural logarithm (base \(e\))with the units <strong>nats</strong>.
        </blockquote>

        <h1>Joint Entropy</h1>
        <blockquote>
            The <strong>joint entropy</strong> between two random variables \(X\) and \(Y\) is defined as
            \[
            \mathbb{H}(X, Y) = - \sum_{x, y} P(x, y) \log P(x, y).
            \]
            If \(X\) and \(Y\) are independent, \(\mathbb{H}(X, Y) =  \mathbb{H}(X) + \mathbb{H}(Y)\). Otherwise, the sum is 
            larger than \(\mathbb{H}(X, Y)\). Also, if \(Y\) is a deterministic function of \(X\), then \(\mathbb{H}(X, Y) = \mathbb{H}(X)\). Thus, 
            \[
            \mathbb{H}(X) + \mathbb{H}(Y) \geq \mathbb{H}(X, Y) \geq \max\{\mathbb{H}(X), \mathbb{H}(Y)\} \geq 0.
            \]
            This is true for more than two random variables. 
        </blockquote>

        <h1>Conditional Entropy</h1>
        <blockquote>
            The <strong>conditional entropy</strong> of \(Y\) given \(X\) is the uncertainty we have in \(Y\) after 
            seeing \(X\), averaged over possible values for \(X\):
            \[
            \mathbb{H}(Y | X) = \mathbb{E }_{P(X)}[ \mathbb{H}(P(Y | X))]
            \]
            <div class="proof">
                <span class="proof-title"> \(\mathbb{H}(Y | X)  = \mathbb{H}(X, Y) -  \mathbb{H}(X)\):</span>
                \[
                \begin{align*}
                \mathbb{H}(Y | X) &=  \mathbb{E }_{P(X)}[ \mathbb{H}(P(Y | X))] \\\\
                                  &=  \sum_{x} P(x) \mathbb{H}(P(Y | X = x)) \\\\
                                  &= - \sum_{x} P(x) \sum_{y} P(y | x) \log P(y | x) \\\\\
                                  &= - \sum_{x, y} P(x, y) \log P(y | x) \\\\
                                  &= - \sum_{x, y} P(x, y) \log \frac{P(x,y)}{P(x)}\\\\
                                  &= - \sum_{x, y} P(x, y) \log P(x,y) + \sum_{x} P(x) \log P(x) \\\\
                                  &= \mathbb{H}(X, Y) -  \mathbb{H}(X) \tag{1}
                \end{align*}
                \]
            </div>
            Note: If \(Y\) can be completely determined by \(X\), \(\mathbb{H}(Y | X)  = 0\). 
            Also, if both \(X\) and \(Y\) are independent each other,  \(\mathbb{H}(Y | X)  =  \mathbb{H}(Y) \).
            <br><br>
            By (1), \(\mathbb{H}(X, Y) = \mathbb{H}(X) + \mathbb{H}(Y | X) \). This implies the <strong>chain rule</strong> of entropy: 
            \[
            \mathbb{H}(X_1, X_2, \cdots, X_n) = \sum_{i =1}^n \mathbb{H}(X_i |X_1, \cdots,  X_{i-1}).
            \]
            
        </blockquote>

        <h1>Cross Entropy</h1>
        <blockquote>
            The <strong>cross entropy</strong> between two discrete distributions \(p\) and \(q\) is defined as
            \[
            \mathbb{H}(p, q) = - \sum_{k=1}^K p_k \log  q_k.
            \]
        </blockquote>

        <h1>KL Divergence (Relative Entropy, Information Gain)</h1>
        <blockquote>
            It is important to define a distance metric to measure "how similar" two distributions \(p\) and \(q\) are. However, 
            instead, we can define a <strong>divergence measure</strong> \(D(p,q)\), which only requires \(D(p, q) \geq 0\) with 
            equality if and only if \(p = q\). Here, we introduce the <strong>Kullback-Leibler divergence</strong> between two 
            distributions \(p\) and \(q\): 
            \[
            \begin{align*}
            D_{\mathbb{KL}}(p \| q) &= \sum_{k =1}^K p_k \log \frac{p_k}{q_K} \\\\
                                   &= \sum_{k =1}^K p_k \log p_k - \sum_{k =1}^K p_k \log q_k \\\\
                                   &= - \mathbb{H}(p) + \mathbb{H}(p, q) 
            \end{align*}
            \]
            where \(p\) and \(q\) are defined on the same sample space \(\mathcal{X}\).
        </blockquote>

        <br><a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>   
    </body>
</html>